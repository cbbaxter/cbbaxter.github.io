<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Caitlin Baxter">
<meta name="dcterms.date" content="2024-05-17">
<meta name="description" content="Audio Emotion Classification">

<title>My Awesome CSCI 0451 Blog - Final Project Blog Post</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: white;
      }

      .quarto-title-block .quarto-title-banner {
        color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
      }
</style>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Awesome CSCI 0451 Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Final Project Blog Post</h1>
                  <div>
        <div class="description">
          Audio Emotion Classification
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Caitlin Baxter </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 17, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="note" class="level3">
<h3 class="anchored" data-anchor-id="note">Note</h3>
<p>The first seven sections of this blog post were completed collaboratively with my partner Otis.</p>
<p>The final personal reflection section was completed individually.</p>
</section>
<section id="abstract" class="level3">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<p>Speech emotion classification is imperative for successful speech recognition since tone can fundamentally change the meaning of a phrase. We attempt to classify the Ryerson Audio-Visual Database of Emotional Speech and Song into 8 different emotions. We successfully improved on the base rate of 16.6% with a CNN model that had a validation accuracy of 39.9%. While this is a significant improvement, our model still displayed overfitting and we hope to improve overall accuracy in the future.</p>
<p>Here is a link to our <a href="https://github.com/Hokalaka2/voice-emotion-classification">source code</a>.</p>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>Speech emotion recognition falls under the category of affective computing which aims to investigate interactions between humans and computers, and inform the optimization of these exchanges (<span class="citation" data-cites="cambria_affective_2017">Cambria et al. (<a href="#ref-cambria_affective_2017" role="doc-biblioref">2017</a>)</span>). While currently there are many different models being developed to address the limitations in the field, some studies have explored the use of Convolutional Neural Networks (CNNs) combined with Mel spectrograms. As Ong et al.&nbsp;writes, CNNs “excel at capturing local patterns and spectral information, but struggle with modeling long-term dependencies and sequential dynamics present in speech signals” (<span class="citation" data-cites="ong_mel-mvitv2_2023">Ong et al. (<a href="#ref-ong_mel-mvitv2_2023" role="doc-biblioref">2023</a>)</span>). In our investigation we sought to examine how a CNN would perform on classifying emotions based on mel-spectrograms with fourier transforms.</p>
<p>Much of the contemporary literature surrounding Sound Classification involves the use of CNNs on Mel spectrograms. In 2019, Zeng et al.&nbsp;used a deep neural network called GResNets to classify emotions using the RAVDESS dataset and recorded an accuracy of 64.48% (<span class="citation" data-cites="zeng_spectrogram_2019">Zeng et al. (<a href="#ref-zeng_spectrogram_2019" role="doc-biblioref">2019</a>)</span>). Their model aimed to expand upon a CNN based model using linear connections to assist with training their CNNss. In 2021, Zhang et. al experimented using the TAU Urban Acoustic Scenes 2019 developmental dataset, and aimed to distinguish between 10 different types of acoustic scenes, such as airports or shopping malls. When inputting the full Mel spectrogram they achieved a highest classification accuracy of 67.53%. They also determined that their model using CNNs and Mel spectrogram feature representations outperformed the existing system of directly inputting an entire Mel Spectrogram by 5.64% (<span class="citation" data-cites="zhang_acoustic_2021">Zhang et al. (<a href="#ref-zhang_acoustic_2021" role="doc-biblioref">2021</a>)</span>). In 2023, Ong et. al.&nbsp;combined Mel spectrograms with Short-Term Fourier Transforms with Multiscale Vision Transformers in order to automatically identify and classify emotions from speech signals. Their model achieved 81.75% classification accuracy on the RAVDESS dataset, which is the same dataset we chose to investigate (<span class="citation" data-cites="ong_mel-mvitv2_2023">Ong et al. (<a href="#ref-ong_mel-mvitv2_2023" role="doc-biblioref">2023</a>)</span>). While much of the research takes slightly different approaches and adds additional hyperparameter tuning and models, a substantial amount of the contemporary literature we encountered involved using Mel Spectrograms and CNNs as a basis for their models. In our investigation we chose to focus on this first step of Speech Emotion classification, in order to develop a better understanding of the field.</p>
<p>There are a number of benefits to being able to classify emotions from audio files, because so much of understanding human speech is dependent on the tone in which it is spoken. Tone provides a contextual basis that allows a person to distinguish between a friendly and joking “I hate you” and an angry and threatening “I hate you”. Things like irony and sarcasm can be difficult to pick up on which can lead to gaps in understanding and conversation. Optimizing a computer’s ability to contextualize the speech it is hearing could lead to technological advancements that also benefit human to human interaction. In 2018, Goy et al.&nbsp;investigated how hearing aids affect a person’s ability to discern emotion through speech. As they write, “Successful communication depends on the perception of nonlinguistic as well as linguistic information” (<span class="citation" data-cites="goy_huiwen_hearing_nodate">Goy Huiwen et al. (<a href="#ref-goy_huiwen_hearing_nodate" role="doc-biblioref">n.d.</a>)</span>). Studies have shown that older adults with hearing loss have more difficulty with auditory emotion perception. Goy et al.&nbsp;determined that despite improving an adult’s ability to understand the phrase said, the amplification of the sound did not allow for significant improvement of emotion-identification accuracy. Work in speech emotion recognition could allow for technological improvements that assist for effective communication between not only humans and machines, but also between humans and is an important area of exploration.</p>
</section>
<section id="values-statement" class="level3">
<h3 class="anchored" data-anchor-id="values-statement">Values Statement</h3>
<p>Our project could overall improve machines that are trying to interpret human emotion. This could be important in any device that uses speech because understanding tone is crucial for understanding human speech. In this way, our model could help anyone who wants to have speech recognition on their devices for ease of use. It could also help blind or seeing impaired people because their speech could be better read by a device. While we’re excited about the prospects of our project, we understand that it will exclude the majority of English speakers and all non-English speakers. This is because our training data only contains American English voices. This means that our models will most likely work significantly worse for people in different countries or with accents. We understand that this may perpetuate existing algorithmic inequities and possible xenophobic or racist attitudes. However with this understanding and the knowledge that we won’t spread our model, we feel comfortable undertaking our project as a learning exercise.</p>
<p>Overall, we believe that our project offers a crucial stepping stone for language comprehension. While our project in its current form will most likely exclude people, it offers a methodology that future research can build on with more comprehensive data. We hope that our project also serves to teach us valuable insights about emotion classifications and audio vectorization.</p>
</section>
<section id="materials-and-method" class="level3">
<h3 class="anchored" data-anchor-id="materials-and-method">Materials and Method</h3>
<p><strong>Data</strong> This investigation relied on the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) (<span class="citation" data-cites="livingstone_ryerson_2018">Livingstone and Russo (<a href="#ref-livingstone_ryerson_2018" role="doc-biblioref">2018</a>)</span>), which we accessed through Zenodo. It contains 24 professional actors (12 male and 12 female) with American accents reciting two identical phrases in a calm, happy, sad, angry, fearful, surprised, or disgusted tone. The data files exist in audio format, and have been named using conventions to reflect other information about the recording. The tone of each file was rated 10 times by 247 other individuals in order to sort them into emotional categories. While the dataset contains additional video and song files, we decided to examine only the audio data files.</p>
<p>The dataset we are using does have a number of limitations in representation. The audio files are all in English and contain recordings of people with American accents, meaning that a huge percentage of the population is not reflected at all.</p>
<p><strong>File Name Parsing</strong> To input the audio files into our model, we needed to parse each data file name and ensure that each file was of the same length. Our method of classification was using a supervised model, meaning that each audio file was already labeled with the correct emotion classification. Our filenames were split into 7 different numbers split by dashest (e.g.&nbsp;“02-01-06-01-02-01-12.mp4”) where the third number represented the emotion that the audio file was trying to represent. To start our project, we had a parse each file into a dataframe that contained the audio and a number 0-7 representing the emotion (since the file names start with emotions at 1 and our model uses the emotions start at 0, the emotion is the third digit minus one).</p>
<p><strong>Audio Parsing</strong> While our audio files initially come in mp4 format, we need to convert this into numbers so that it can be successfully used in our model. Using the Librosa library, we apply a Fourier transform on the audio file. A Fourier transform is a mathematical formula that converts an audio signal from the time domain into the frequency domain, reducing the signal to its individual frequencies and their amplitudes. This allows us to analyze the frequencies rather than the time of the audio file. This creates something called a spectrogram. While we could use this to train our model, it wouldn’t be very effective because humans don’t actually hear sound on a linear scale. Humans tend to hear lower frequencies better than higher frequencies. To account for this we can change our data to a mel scale, making our data a mel spectrogram. In Fig 1, we can see what this looks like.</p>
<p>Fig 1: <img src="../../img/output.png" class="img-fluid" alt="Alt text"></p>
<p>Fig 1 shows the first 12 mel spectrograms and their respective labels. We might notice varying degrees of black space at the start or the end of our files, this is because we had to pad the files to make them all the same length. We were concerned about the model using the amount of black space in the beginning as a way to classify a certain emotion so we randomly allocated black space to the front and the back.</p>
<p><strong>CNN Model</strong> We chose to use a Convolutional Neural Network to classify our mel spectrograms. This is similar to what we saw in related research papers. While figuring out the exact amount of layers to have was difficult, we eventually settled on a pipeline with 2 2D convolutions and 2 2x2 MaxPools with ReLU’s separating each convolution. This pipeline can more easily be seen in Fig 2.</p>
<p>Fig 2: <img src="../../img/CNN_model.png" class="img-fluid" alt="Alt text"></p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>After running our model for 50 epochs, we found that the model performed with a 39.9% validation accuracy which is significantly higher than our base rate of 16.6%. In terms of our testing data set of size 201 audio files, this means that the base rate would correctly predict around 30 audio files while our model would predict around 72 audio files correctly. This is about a 42% increase in the number of correctly predicted audio files.</p>
<p>In observing the confusion matrix of the classification results from the final epoch, we are able to visualize how the model’s predictions compared to the actual emotion values.</p>
<p>Figure 1: Confusion Matrix Comparing the 75th Epoch’s Model Predictions with the Actual Emotion Classifications of our Validation Data Table 1: <img src="../../img/confusion_matrix.jpg" class="img-fluid" alt="Alt text"></p>
<p>As seen in Figure 1, the model was much more likely to classify an audio file as certain emotions than others. The model underpredicted emotions such as Fearful which made up 12.5% of the test data, but was only predicted 6.60% of the time, and overpredicted others such as Happy which only makes up 12.8% of the test data, but was predicted 21.2% of the time.</p>
<p>An interesting observation demonstrated in the confusion matrix is some of the misclassifications. The model misclassified Neutral audio files as Calm most often, which are emotions that many people might consider fall into the same domain. Similarly, emotions like ‘Angry’, ‘Disgust’, and ‘Happy’, which are all elevated responses, were more likely to be misclassified as one another.</p>
<p>Table 1: <img src="../../img/table_final.png" class="img-fluid" alt="Alt text"></p>
<p>Despite the model successfully improving the base rate, we believe that the model is underperforming because of overfitting. Our training accuracy was 92.2% which is significantly higher than our validation accuracy of 39.9%. Ideally the training accuracy and the validation accuracy would be much more similar.</p>
<section id="concluding-discussion" class="level3">
<h3 class="anchored" data-anchor-id="concluding-discussion">Concluding Discussion</h3>
<p>Our primary goal for this project was to create a model that correctly classifies the emotional tone of an audio file with an accuracy greater than randomly assigning categories, which we achieved. Our model doubled the accuracy over the baseline and demonstrated its effectiveness at classifying emotions. We created a Jupyter notebook, linked at the beginning of this blog post, where we were able to vectorize and visualize the audio files. We were able to gain a strong understanding of audio analysis and better comprehend how our neural network behaves.</p>
<p>In comparison to the contemporary literature on this subject, our model did not perform with the same level of accuracy when classifying emotions. Many of their models performed with a classification accuracy between 60-80%, whereas ours was approximately 40%. Given that we were performing the earlier stages of experimentation, we feel that our model performed quite well comparatively. Due to time and computational power constraints we were also only able to perform 50 epochs and had to keep the layers of our CNN rather small. If we had more time to improve our project we would work to extract features from the mel spectrogram in order to ideally improve the effectiveness of our CNN. Zhang et. al.&nbsp;write that the input to CCNs should be simple, as it is challenging for CNNs to extract all of the features of the Mel Spectrogram at once (<span class="citation" data-cites="zhang_acoustic_2021">Zhang et al. (<a href="#ref-zhang_acoustic_2021" role="doc-biblioref">2021</a>)</span>).</p>
</section>
<section id="group-contributions-statement" class="level3">
<h3 class="anchored" data-anchor-id="group-contributions-statement">Group Contributions Statement</h3>
<p>The majority of work on this project was done collaboratively by peer programming or live sharing on VSCode. We occasionally did work apart but this was mostly isolated to research and running the model with different parameters. Overall, we feel pretty comfortable in the breakdown of the work in this project.</p>
</section>
<section id="personal-reflection" class="level3">
<h3 class="anchored" data-anchor-id="personal-reflection">Personal Reflection</h3>
<p>I learned a lot over the course of this project. Firstly, I feel that I greatly improved my programming skills by learning how to analyze auditory data and implementing a Convolutional Neural Network. I also felt that I developed my ability to both manage and communicate about such a large project. When we were starting out, the scope of the project felt very large and it was a little difficult to figure out what we were supposed to be doing or starting with. I think that allowed me to develop my planning and project management skills as we had to take some time to map out what we were going to work on. We also were in almost constant communication, meeting several times a week to work on the project together and would text one another to keep them updated if we did any work on our own.</p>
<p>I feel pretty good about what we achieved, as out primary goal was to build a model that outperformed the base rate, which it did! That being said, our accuracy was only 40% so had we had more time I would have wished to improve upon that, as our model still gets more wrong than it gets right. In terms of my other goals, I was really happy with the project that we selected because I found it challenging, but also very interesting and fulfilling.</p>
<p>I think that one thing that will stick with me from working on this project was collaborating with Otis and learning how to successfully develop as a team. I also feel that my experience in researching contemporary studies and using that to inform the decisions I make about how to handle data and my model decisions. I feel more comfortable tackling machine learning projects and think that in future I will be more prepared when doing larger scale projects or working with data that I am unfamiliar with.</p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-cambria_affective_2017" class="csl-entry" role="listitem">
Cambria, Erik, Dipankar Das, Sivaji Bandyopadhyay, and Antonio Feraco. 2017. <span>“Affective <span>Computing</span> and <span>Sentiment</span> <span>Analysis</span>.”</span> In <em>A <span>Practical</span> <span>Guide</span> to <span>Sentiment</span> <span>Analysis</span></em>, edited by Erik Cambria, Dipankar Das, Sivaji Bandyopadhyay, and Antonio Feraco, 1–10. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-55394-8_1">https://doi.org/10.1007/978-3-319-55394-8_1</a>.
</div>
<div id="ref-goy_huiwen_hearing_nodate" class="csl-entry" role="listitem">
Goy Huiwen, M. Kathleen Pichora-Fuller, Gurjit Singh, and Frank A. Russo. n.d. <span>“Hearing <span>Aids</span> <span>Benefit</span> <span>Recognition</span> of <span>Words</span> in <span>Emotional</span> <span>Speech</span> but <span>Not</span> <span>Emotion</span> <span>Identification</span>.”</span> Accessed May 6, 2024. <a href="https://journals.sagepub.com/doi/pdf/10.1177/2331216518801736">https://journals.sagepub.com/doi/pdf/10.1177/2331216518801736</a>.
</div>
<div id="ref-livingstone_ryerson_2018" class="csl-entry" role="listitem">
Livingstone, Steven R., and Frank A. Russo. 2018. <span>“The <span>Ryerson</span> <span>Audio</span>-<span>Visual</span> <span>Database</span> of <span>Emotional</span> <span>Speech</span> and <span>Song</span> (<span>RAVDESS</span>): <span>A</span> Dynamic, Multimodal Set of Facial and Vocal Expressions in <span>North</span> <span>American</span> <span>English</span>.”</span> <em>PLOS ONE</em> 13 (5): e0196391. <a href="https://doi.org/10.1371/journal.pone.0196391">https://doi.org/10.1371/journal.pone.0196391</a>.
</div>
<div id="ref-ong_mel-mvitv2_2023" class="csl-entry" role="listitem">
Ong, Kah Liang, Chin Poo Lee, Heng Siong Lim, Kian Ming Lim, and Ali Alqahtani. 2023. <span>“Mel-<span>MViTv2</span>: <span>Enhanced</span> <span>Speech</span> <span>Emotion</span> <span>Recognition</span> <span>With</span> <span>Mel</span> <span>Spectrogram</span> and <span>Improved</span> <span>Multiscale</span> <span>Vision</span> <span>Transformers</span>.”</span> <em>IEEE Access</em> 11: 108571–79. <a href="https://doi.org/10.1109/ACCESS.2023.3321122">https://doi.org/10.1109/ACCESS.2023.3321122</a>.
</div>
<div id="ref-zeng_spectrogram_2019" class="csl-entry" role="listitem">
Zeng, Yuni, Hua Mao, Dezhong Peng, and Zhang Yi. 2019. <span>“Spectrogram Based Multi-Task Audio Classification.”</span> <em>Multimedia Tools and Applications</em> 78 (3): 3705–22. <a href="https://doi.org/10.1007/s11042-017-5539-3">https://doi.org/10.1007/s11042-017-5539-3</a>.
</div>
<div id="ref-zhang_acoustic_2021" class="csl-entry" role="listitem">
Zhang, Tao, Guoqing Feng, Jinhua Liang, and Tong An. 2021. <span>“Acoustic Scene Classification Based on <span>Mel</span> Spectrogram Decomposition and Model Merging.”</span> <em>Applied Acoustics</em> 182: 108258. https://doi.org/<a href="https://doi.org/10.1016/j.apacoust.2021.108258">https://doi.org/10.1016/j.apacoust.2021.108258</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>