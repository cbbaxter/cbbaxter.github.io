[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The quick brown fox jumped over the lazy dog."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/implementing-the-perceptron-alg/index.html",
    "href": "posts/implementing-the-perceptron-alg/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/implementing-the-perceptron-alg/index.html#abstract",
    "href": "posts/implementing-the-perceptron-alg/index.html#abstract",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post I implemented the perceptron alogorithm to classify linearly seperable data. It was demonstrated through experimentation that the algorithm will terminate with a loss of zero on linearly seperable data, but will never reach a loss of zero and either not terminate, or reach a maximum number of iterations on non-linearly seperable data. The time complexity of the algorithm is dependent on the number of data points and features."
  },
  {
    "objectID": "posts/implementing-the-perceptron-alg/index.html#perceptron-implementation",
    "href": "posts/implementing-the-perceptron-alg/index.html#perceptron-implementation",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Perceptron Implementation",
    "text": "Perceptron Implementation\nHere is the link to my perceptron implementation.\nHere is my implementation of the perceptron.grad() function:\ndef grad(self, X, y):\n    score = self.score(X)\n    return torch.where((y * score) &lt; 0, y @ X, 0.0)   \nThe gradient function begins by calculating the score of model, using the following formula to calculate the dot product between the vector \\(w\\) and \\(X_i\\)\n\\[s_i = \\langle w, X_i\\rangle\\]\nThe gradient is then calculated by the following formula:\n\\[{1}\\left[s_i y_i &lt; 0 \\right] y_i \\ x_i\\]\nIn my implementation I used torch.where() to replicate the indicator function (fancy \\(1\\)). The first parameter of the torch.where() function checks the condition that the score muliplied by y is greater than zero. If it is, the function returns the dot product of \\(y\\) and \\(X\\), and if not it returns 0.\n\n# Generate Data\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2 * y - 1\n    y = y.type(torch.FloatTensor)\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\nfig, ax = plt.subplots(1, 1, figsize= (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nThis code, by Professor Philip Chodrow, generates linearly seperable data which is neccessary to test my impletation of the perceptron algorithm. The plot shows the data, which we can clearly see is clumped in two different categories that could be seperated by a straight line.\n\nPart A\n\n# Check your Implementation \ntorch.manual_seed(1234)\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter = 1000\n\nwhile loss &gt; 0 and max_iter &gt; 0:\n    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    max_iter = max_iter - 1\n\nIn this code block, we use perceptron on the generated linearly seperable data to test its implementation. A while loop runs while the loss value is greater than one, or until the maximum number of iterations is reached. The loss is calculated at the beginning of each iteration. Then, a random data point is selected and a perceptron update is performed using the random data point. A perceptron step once again calculates the loss using the randomly selected data point, then uses Perceptron’s grad() function to determine the change to the vector \\(w\\). Once the loss reaches 0 or the maximum number of iterations is reached, the loop will terminate.\n\nplt.plot(loss_vec, color = \"skyblue\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"skyblue\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Loss over Perceptron Interations for Linearly-Seperable Data\")\n\n\n\n\n\n\n\n\nThis plot, showing the loss of the function at different iterations demonstrates the efficacy of my implementation of the perceptron algorithm since it terminates after approximately 130 iterations. This means that the algorithm is able to determine a linear boundary that allows for 0 loss before the algorithm reaches 1000 iterations.\n\n\nPart B : Experiments\n\n# Generate Data Functions\n# by Prof. Phil Chodrow\n\ndef perceptron_data(n_points, noise, p_dims):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2 * y - 1\n    y = y.type(torch.FloatTensor)\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\n\n# Generate 2D Linearly Seperable Data\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims = 2)\n\nfig, ax = plt.subplots(1, 1, figsize= (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nSimilarily to Part A, we can see in this plot a visualization of linearly seperable data.\n\ntorch.manual_seed(1234)\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1.0\nloss_vec = []\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nmax_iter = 1000\ncurr_iter = 0\n\nwhile loss &gt; 0 and curr_iter &lt; max_iter:\n    ax = axarr.ravel()[current_ax]\n    \n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    i = torch.randint(n, size = (1,))\n\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = opt.step(x_i, y_i)\n\n    \n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n    \n    curr_iter = curr_iter + 1\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nIn this plot, we can see the evolution of the loss function during training, and the seperating line when the model reaches a loss of zero. In each panel of the plot, we are able to see the loss, the decision boundary that led to that loss and the previous decision boundary. We can see how the loss changed over the course of the iterations, until finally a loss of zero was reached. If this data were not linearly seperable the function would have terminated before the loss function was zero and the final panel would have have had a loss &gt; 0.0.\n\n# Generate 2D Non-Linearly Seperable Data\n# Code modelled from Prof. Phil Chodrow\ntorch.manual_seed(1234)\n\nX, y = perceptron_data(n_points = 300, noise = 0.9, p_dims = 2)\n\nfig, ax = plt.subplots(1, 1, figsize= (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\nBy increasing the noise of the generated perceptron data, we created data that is not linearly-seperable. As visualized in this plot, the data does not appear to be linearly seperable, as the two points overlap pretty significantly and it is not possible to draw a straight line through the two. When use our perceptron on this data we expect it not to reach a loss of zero.\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1.0\nloss_vec = []\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nmax_iter = 1000\ncurr_iter = 0\n\nfinal_w = torch.clone(p.w)\n\nwhile loss &gt; 0 and curr_iter &lt;= max_iter: #and max_iter &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    curr_iter = curr_iter + 1\n    if curr_iter == max_iter:\n        final_w = torch.clone(p.w)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(final_w, x_min = -2, x_max = 4, ax = ax, color = \"black\")\nax.set_ylim(-2, 4)\n\n\n\n\n\n\n\n\n\n\nThis plot shows the decision boundary of the final iteration. As evidenced in the plot, there and blue and orange points on either side of the boundary, demonstrating that the data is not seperated by the linear line and therefore not linearly seperable.\n\nplt.plot(loss_vec, color = \"skyblue\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"skyblue\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Loss over Perceptron Interations for Non-Linear Data\")\n\n\n\n\n\n\n\n\nThis plot shows the evolution of the loss over training. It does not terminate until it has reached max_iter which makes sense as the data is not linearly seperable.\n\n# Generate 5 feature Data\n# Code modelled from Prof. Phil Chodrow\ntorch.manual_seed(1234)\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims = 5)\n\ntorch.manual_seed(1234)\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter = 1000\n\nwhile loss &gt; 0: #and max_iter &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    max_iter = max_iter - 1\n\n\nplt.plot(loss_vec, color = \"skyblue\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"skyblue\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Loss over Perceptron Interations for 5-feature Data\")\n\n\n\n\n\n\n\n\nThe results of this table indicate that the 5-feature data is linearly seperable because the Perceptron terminates after about 800 iterations instead of reaching max_iter."
  },
  {
    "objectID": "posts/implementing-the-perceptron-alg/index.html#discussion-of-runtime",
    "href": "posts/implementing-the-perceptron-alg/index.html#discussion-of-runtime",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Discussion of Runtime",
    "text": "Discussion of Runtime\nDuring a single iteration of the perceptron algorithm, the runtime is \\(O(n \\times p)\\), where \\(n\\) is the number of data points and \\(p\\) is the number of features. This is due to the calculation of the score function which requires the multiplication of \\(X\\) and \\(w\\) to be calculated. \\(X\\) is size \\(n \\times p\\), as each data point will have \\(p\\) features. Meanwhile \\(w\\) is size \\(n\\).\nTo compute each row of the resulting matrix, takes \\(O(n)\\) time as it requires \\(n\\) multiplcations and \\(n-1\\) additions. As there are \\(p\\) rows in matrix \\(X\\), the total number of operations requires a time complexity of: \\[O(n \\times p)\\].\nThis means that the runtime complexity of a single iteration depends both on the number of features and data points in \\(X\\)."
  },
  {
    "objectID": "posts/implementing-the-perceptron-alg/index.html#conclusion",
    "href": "posts/implementing-the-perceptron-alg/index.html#conclusion",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Conclusion",
    "text": "Conclusion\nThrough this assignment, I was able to learn more about Perceptron and Gradient Descent. Through the experimentation section, I observed that the Perceptron algorithm is only effective when the data is linearly seperable. Its runtime is also dependent on the number of points and number of features in the dataset."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n# Load Data\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\nAbstract\nIn this blog post I investigated the classification of Gentoo, Adelie, and Chinstrap penguins. It was determined that using Logistic Regression with the variables culmen length, culmen depth, and the island where the penguins are found allowed for the correct classification of penguin 100.0% of the time on the test data. The culmen is the dorsal ridge atop the bill of the penguin, and its depth and length varies between the three species.\n\nFigure 1: Adelie penguins\nimage source: https://i.natgeofe.com/n/00c35612-9827-40d9-a04e-6079806dca52/3798855_2x1.jpg\n\nFigure 2: Chinstrap penguin\nimage source: https://live.staticflickr.com/707/31973598931_6d9bfd3f18_b.jpg\n\nFigure 3: Gentoo penguin\nimage source: https://cdn.britannica.com/08/152708-050-23B255B3/Chinstrap-penguin.jpg\nLogistic regression was compared with Decision Tree Classification and Support Vector Classification and was determined to be the most accurate model when the C value was set to 17.\n\n# Data Preparation\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nsummary_table = train.groupby([\"Species\", \"Island\"])[\"Body Mass (g)\"].mean()\nprint(summary_table)\n\nSpecies                                    Island   \nAdelie Penguin (Pygoscelis adeliae)        Biscoe       3711.363636\n                                           Dream        3728.888889\n                                           Torgersen    3712.804878\nChinstrap penguin (Pygoscelis antarctica)  Dream        3743.421053\nGentoo penguin (Pygoscelis papua)          Biscoe       5039.948454\nName: Body Mass (g), dtype: float64\n\n\nFrom this summary table it is evident that Chinstrap penguins only live on the Dream island while Gentoo penguins only live on Biscoe. Meanwhile, Adelie Penguins live on Biscoe, Dream, adn Torgersen. This suggests that the Island where an individual is located may be a good variable for a machine learning algorithm as we know that if an individual is located on torgersen they must be an Adelie and that if an individual is found on Dream it is not a Gentoo penguin.\n\nmass_table = sns.relplot(data = train, x = \"Body Mass (g)\" , y = \"Flipper Length (mm)\", hue = \"Species\")\nmass_table.set_axis_labels(\"Body Mass (g)\", \"Flipper Length (mm)\")\nmass_table.add_legend(frameon=True)\nmass_table.legend.set_bbox_to_anchor((1.5, .6))\nplt.gca().set(title = \"Comparing the Body Mass and Flipper Length of Penguins\")\nprint(mass_table)\n\n&lt;seaborn.axisgrid.FacetGrid object at 0x12f8a37f0&gt;\n\n\n\n\n\n\n\n\n\nFrom this figure it is apparent that the flipper length of a penguin is positively related with their body mass. It is clear that Gentoo penguins are generally larger than chinstrap and adelie penguins, and the latter two are relatively similar in size.\n\nflipper_table = sns.relplot(data = train, x = \"Flipper Length (mm)\" , y = \"Culmen Length (mm)\", hue = \"Species\", style = \"Island\")\nflipper_table.set_axis_labels(\"Flipper Length (mm)\", \"Culmen Length (mm)\")\nflipper_table.add_legend(frameon=True)\nplt.gca().set(title = \"Comparing the Culmen Length and Flipper Length of Penguins\")\nflipper_table.legend.set_bbox_to_anchor((1.5, .6))\nprint(flipper_table)\n\n&lt;seaborn.axisgrid.FacetGrid object at 0x154b5fac0&gt;\n\n\n\n\n\n\n\n\n\nIn this plot we can see that while Adelie and Chinstrap penguins have similar Flipper lengths, Chinstrap penguins generally have longer culmens. The flipper and culmen length of Adelie penguins does not appear to vary greatly depending on the island on which they were found.\n\nculmen_table = sns.relplot(data = train, x = \"Culmen Depth (mm)\" , y = \"Culmen Length (mm)\", hue = \"Species\", style = \"Island\")\nculmen_table.set_axis_labels(\"Culmen Depth (mm)\", \"Culmen Length (mm)\")\nculmen_table.add_legend(frameon=True)\nplt.gca().set(title = \"Comparing the Culmen Length and Culmen Depth of Penguins\")\nculmen_table.legend.set_bbox_to_anchor((1.5, .6))\nprint(culmen_table)\n\n&lt;seaborn.axisgrid.FacetGrid object at 0x154959040&gt;\n\n\n\n\n\n\n\n\n\nIn this plot, we observe that the culmen length and culmen depth seem to be generally distinct between species. Through this preliminary analysis, I decided to focus my search on looking at the combination of qualitative variables with the quantitative variables that relate to the size of the penguins, as they appear to vary in body mass, flipper length, and the size of their culmens.\n\nfrom itertools import combinations\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\ndef logisticRegression(X_train, y_train):\n  best_score = 0\n  best_c = 0\n  for c in range(1, 20): \n    LR = LogisticRegression(max_iter = 5000, C = float(c))\n    LR.fit(X_train, y_train)\n    cv_scores_LR = cross_val_score(LR, X_train, y_train, cv = 5)\n    if cv_scores_LR.mean() &gt; best_score:\n      best_score = cv_scores_LR.mean()\n      best_c = c\n  return cv_scores_LR.mean(), best_c\n\ndef decisionTree(X_train, y_train):\n  best_score = 0\n  best_max_depth = 0\n  for i in range(1, 100):\n    DT = DecisionTreeClassifier(max_depth = i)\n    DT.fit(X_train, y_train)\n    cv_scores_DT = cross_val_score(DT, X_train, y_train, cv = 5)\n    if cv_scores_DT.mean() &gt; best_score:\n      best_score = cv_scores_DT.mean()\n      best_max_depth = i\n  return best_score, best_max_depth\n\ndef svc(X_train, y_train):\n  xscaler = preprocessing.StandardScaler().fit(X_train)\n  X_scaled = xscaler.transform(X_train)\n  gammas = 10**np.arange(-5, 5)\n  best_score = 0\n  best_gamma = gammas[0]\n  for gamma in gammas:\n    svc = SVC(gamma = gamma)\n    svc.fit(X_scaled, y_train)\n    cv_scores_svc = cross_val_score(svc, X_scaled, y_train, cv = 5)\n    if cv_scores_svc.mean() &gt; best_score:\n      best_score = cv_scores_svc.mean()\n      best_gamma = gamma\n  return best_score, best_gamma\n\nbest_vars = []\nmax_score = 0\nmodelType = \"\"\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    select_X_train = X_train[cols]\n    \n    # -- Logistic Regression --\n    LR_score, c = logisticRegression(select_X_train, y_train)\n    if LR_score &gt; max_score:\n      max_score = LR_score\n      best_vars = cols\n      modelType = \"LR\"\n      print(max_score, best_vars, modelType, c)\n\n    # -- Decision Tree -- \n    DT_score, max_depth = decisionTree(select_X_train, y_train)\n    if DT_score &gt; max_score:\n      max_score = DT_score\n      best_vars = cols\n      modelType = \"DT\"\n      print(max_score, best_vars, modelType, max_depth)\n   \n    # -- SVC --\n    svc_score, gamma = decisionTree(select_X_train, y_train)\n    if svc_score &gt; max_score:\n      max_score = svc_score\n      best_vars = cols\n      modelType = \"svc\"\n      print(max_score, best_vars, modelType, gamma)\n\nprint(best_vars, max_score, modelType) \n\n\n0.9530920060331824 ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)'] LR 1\n0.9883107088989442 ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] LR 1\n0.9883861236802414 ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)'] LR 18\n1.0 ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] LR 17\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 1.0 LR\n\n\nSimilar to my observations during my initial investigation into the different variables, we see that Island, Culment Length, and Culmen Depth were used in the most accurate model. The Linear Regression model outperformed both the SVC and the Decision Tree. It was determined that when C = 17, the model was 100% accurate. The C is the inverse regularization strength, and the smaller the value specifies stronger regularization (sklearn documentation). C being 17 indicates a weaker regularization and suggests that the model may be focusing on minimizing error in the training data and may not regularize as well. I decided to test the model again with more cross validation.\n\nX_select = X_train[['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']]\nLR = LogisticRegression(max_iter = 5000, C = 1.0)\nLR.fit(X_select, y_train)\ndefault_c_cv_scores_LR = cross_val_score(LR, X_select, y_train, cv = 5)\ndefault_c_cv_scores_LR.mean()\n\nLR = LogisticRegression(max_iter = 5000, C = 17.0)\nLR.fit(X_select, y_train)\ncv_scores_LR = cross_val_score(LR, X_select, y_train, cv = 5)\ncv_scores_LR.mean()\n\nprint(default_c_cv_scores_LR.mean())\nprint(cv_scores_LR.mean())\n\n0.9843891402714933\n1.0\n\n\nWhen I re-ran the Logistic Regression Model with cross validation with the default value of C vs the one that was found to be most effecient in the above function, it is clear that the Logistic Regression model is already fairly accurate at classifying types of penguins using Culmen Depth, Culmen Length, and the Island that they are found.So I decided to test the LR model on the test data.\n\n# Testing the Model on Test Data\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ncols = ['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nX_test, y_test = prepare_data(test)\n\nX_select = X_train[['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']]\nLR = LogisticRegression(max_iter = 5000, C = 1.0)\nLR.fit(X_select, y_train)\n\ny_preds = LR.predict(X_test[cols])\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\n\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_preds, normalize = \"true\")\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\nAs shown in the above confusion matrix the model accurately predicted the species of penguin each time when looking at the Island, the Culmen Length, and Culmen Depth.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\n\nDiscussion\nIt was determined that when classifying Chinstrap, Gentoo, and Adelie penguins, looking at the length and depth of their culmen along with the island where they are found. The model was able to correctly classify the type of penguin 100% of the time. Through a preliminary investigation of the data, it was evident that Island would likely be a helpful variable in determining the species of the penguins. This was supported when Island was selected as one of the variables in the three variable combination for the best accuracy in the for loop. In the confusion matrix, it is evident that all three types of penguins were correctly identified in the test dataset.\nIn this blog post I learned the importance of investigating different values of parameters and how they affect the model. When I was looping through the different variables, I had the script print out the combinations of variables, their accuracy, and the type of model so I could see how it ultimately found the best one. It was interesting to see how changing the value of C in Logisitic Regression could change the model’s ability to accurately classify. For example moving C = 16 to C = 17, boosted the accuracy from 99.2% to 100.0%"
  },
  {
    "objectID": "posts/women-in-data-science/index.html",
    "href": "posts/women-in-data-science/index.html",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "Abstract\nOn March 4, 2024 I attended Middlebury College’s Women in Data Science Conference that aimed to highlight the work of women using data across different disciplines. Part A of this blogpost uses Corbett and Hill’s report “Solving the Equation: The Variables for Women’s Sucess in Engineering and Computing” to discuss the issues facing female representation in the fields of Engineering and Computing and how events that spotlight women can have a hugely positive effect on breaking down biases and building community. In Part B, I reflect on the conference itself, discsussing the different lightning presentations by Middlebury professors and the keynote speaker, Sarah Brown.\n\nPart A: Why Spotlight Women in Data Science?\nWhy is it a problem that women are underrepresented in computing, math, and engineering? For whom is it a problem?\nThe underrepresentation of women in computing, math, and engineering is not only a problem for many women in those fields, but also contributes to larger societal injustices and problems. Diversity in the workforce not only contributes to creativity, productivity, and innovation, but by excluding approximately half of the population from engineering and technical decisions societal inequalities are often perpetuated. This winter, I began reading a book called Invisible Women: Data Bias in a World Designed for Men by Caroline Criado-Perez that discussed the disparity in the data that has led to many of accepted societal designs and standards. In it, they include a quote from Simone de Beauvoir’s The Second Sex, that states “Representation of the world, like the world itself, is the work of men; they describe it from their own point of view, which they confuse with the absolute truth”. This idea is highlighted throughout Corbett and Hill’s discussion of the negative impact of the lack of representation of women in engineering and computing fields in their report “Solving the Equation: The Variables for Women’s Success in Engineering and Computing”. The lack of representation also makes it more difficult for women who are entering the field. With a lack of role models or representation in high power positions, it can be difficult for women to feel confident or like they belong in computing or engineering fields. This in-turn continues to propagate and negatively reinforce the issues surrounding the underrepresentation in the first place.\nHow is the representation and status of women in computing today different from the 1950s and 1960s? What are some of the forces that brought this change?\nSince the 1950s and 60s women have made substantial advances in many STEM fields, making up 39% of chemists in 2013 as opposed to 8% in 1960. In Computing fields however, women make up just over a quarter of the workforce which is approximately the same as it was in 1960. Only 12% of engineers are women, which is a substantial increase from the 1% in 1960, yet still remains a strikingly small percentage. An important way to facilitate the increase of female representation is by changing the environment. Corbett and Hill discuss how Harvey Mudd College was able to increase the percentage of women graduating from its computing program from 12% to 40% in 5 years by making three changes that made their courses a more welcoming and accessible environment for female computer scientists. Recognizing that the problem not only lies with attracting women to a program, but also keeping them invested and involved, is another essential element of representation.\nWhich of the barriers and unequal challenges described in the section “Why So Few” can be eroded by events that spotlight the achievements of women in STEM?\nA large barrier for women in Computing fields is the lack of a perceived community, both in their role, which is stereotypically often perceived as very solitary, and as an underrepresented group of people. Women are more likely than men to express an interest in work with a clear social purpose and as Corbett and Hill write, “Incorporating communal aspects - both in messaging and substance - into engineering and computing work will likely increase the appeal of these fields to communally oriented people, many of whom are women.” Events that highlight the success of women also help cultivate a sense of belonging which is essential for a field or workplace when trying to attract and retain diversity. Through events that spotlight the achievements of women, we are able to also help erode implicit gender biases and stereotypes. Corbett and Hill discuss how exposing students to female role models early in life might be able to change the implicit biases between math capabilities and gender.\n\n\nPart B: Middlebury Women in Data Science Conference\nMiddlebury professor Amy Yuen presented on ‘Is the UN Security Council a Democratic Institution?’ and discussed how she has used data in her research on the UN’s security council and how the status of different members effect the resolutions and types of meetings the council holds. The UN Security council is comprised of 15 members, with 5 permanent members who hold veto power, and 10 elected members that serve 2-year terms. In measuring the different resolutions and meetings brought forward by the permanent members versus the elected members, Professor Yuen looked to access how the different elected members affect the council. They found differences in sponsorship of resolutions, determining that permanent members often put out the most resolutions that relate to current events while elected members focus on more thematic issues such as climate change. They found that the actual line of best fit of their data fit closer to their modelled ‘most inclusive cycle’ which suggests that while the Security Council is definitely not democratic in the way we often think of it, it is fairly representative of the world scene and engages more states than people would imagine. I learned a lot about how the UN Security Council functions from this presentation and also a fun example of how data is used outside of Computer Science research. One thing that I found interesting was the challenges of aquiring data. Professor Yuen discussed how even though all of this data is public record and has been well-documented, the data provided by the UN was at times incorrect and had to be re-checked by her research assistants.\nGeography professor Dr. Jessica L’Roe presented on her research on deforestation. She showed us several of her projects, and discussed how she and her research assitance both collected data and analyzed it. One of the first examples she discussed was in Brazil, where while the government was very good at detecting deforestation, they found it difficult to actually identify who was responsible for clearing land. The government created a new initiative that had people register their land, but the self-reports provided by people were often incorrect as people would lie about the size of their land. So while they have a lot of data from aerial observation and statistics, a lot of their research relies on interviewing other people. She echoed this in another example of her research in Uganda. I found her discussion of how interviewing of locals provided them with essential information in wildlife and woodlots conservation. I think that sometimes in Computer Science and Statistics data can feel really impersonal and it was nice to bring the discussion back to the people who are the basis of and are greatly affected by models.\nFinally, Professor Laura Biester presented on her research using Computation Linguistic Models to study Mental Health issues. Research into mental health involves the fundamental challenge of access to quality data due to the privacy protections in place for health data, so many researchers rely on self-reports where an individual has posted on the internet about their diagnosis. Typically, models built on self-report datasets do not generalize as well, so Professor Biester and her colleagues turned to Reddit. Looking at over 360 JSON files with all of the Redditposts and comments from 2006-2019, they developed a dataset with 20k diagnosed users and controls then used Bidirectional Encoder Representations from Transformers (BERT) to extract contextual representations of words that were associated with the lead-up to a depression diagnosis. I learned a lot about how these large language model studies are conducted through Professor Biester’s presentation. I found her discussion on the limitations of Reddit data as it includes very little demographic information about the individuals to be interesting, and made me wonder at how the results might vary if we had information about the gender of the user. How would the terminology vary for a female identifying, or non-binary individual? I found Professor Biester’s discussion on how she uses huge amounts of data to investigate language to be very interesting and informative.\nProfessor Sarah Brown, from the University of Rhode Island, was the main keynote speaker of the event. She presented on ‘Data Science Skills in Unexpected Places’ and discussed different non-technical skills that she has aquired over the years that help her solve data science problems and overcome roadblocks when faced with an issue. One of her main points that I found particularly interesting was her discussion of data as a primary source that needs to be assessed within its context in order to be properly understood. She began her presentation by showing a three circled-venn diagram that had ‘Computer Science’, ‘Domain Experts’, and ‘Statistics’ in each circle, with the overlapping section highlighted as ‘Data Science’. She discussed how each discipline is not simply a topic, but rather a community full of people, norms, and vocabularies that need to be understood in order to properly analyze its data.\nProfessor Brown also discussed the question of accuracy versus fairness, suggesting a change from the traditional data pipeline to incorporate the question of fairness into the earlier stages when one is exploring the data and variables:\ncollect -&gt; clean -&gt; explore -&gt; fair? -&gt; model -&gt; deploy\nShe discussed an example in healthcare where by changing the target variable of the model, they were able to make it more equitable. Computer Science is a novice field with disproportionate power, which can lead to chaos. By incorporating discussions of fairness and auditing our models into our processes we are able to work on addressing bias in models as we are going instead of realizing after deployment that it is behaving problematically. One thing that I found particularily impactful was when Professor Brown stated how we can have an incredible accurate algorithm that is biased, because the world is biased and therefore the data generated by the world is biased.\nReflection\nAs I was scrolling through Instagram the other day I came across a commercial that was made by Mercedes-Benz for International Women’s Day in 2023. The video is narrated by a few young girls and the overall premise is that they are not interested in being ‘exceptional’ because they are the first woman to do something, they would rather be one of many. I think that messaging really highlights what I have been reflecting on throughout this blogpost - how the underrepresentation of women in Computing and Engineering fields can feel like a self-perpetuating cycle, and that by highlighting and empowering successful women in these fields we are able to build representation and community. I had a great time at Middlebury’s Women in Data Science conference and felt that I both learned a lot, and also got to meet a lot of other students through their ‘Get to know you bingo activity’ and left happily with a midd data tshirt."
  },
  {
    "objectID": "posts/replication-study/index.html",
    "href": "posts/replication-study/index.html",
    "title": "Optimal Decision Making",
    "section": "",
    "text": "# Part 1: Data Access\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\n# Part B: Reproduce Figure 1\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nb_subset = df.loc[df[\"race\"] == \"black\"]\nb_percentiles = np.percentile(b_subset[\"risk_score_t\"].values, np.arange(0, 100))\n\nw_subset = df.loc[df[\"race\"] == \"white\"]\nw_percentiles = np.percentile(w_subset[\"risk_score_t\"].values, np.arange(0, 100))\n\nb_mean_c_i = []\nw_mean_c_i = []\n\nfor i in range(0, 100):\n    b_indices = np.where(b_subset[\"risk_score_t\"].values &gt;= i & b_subset[\"risk_score_t\"].values &lt; i+1)\n    b_mean = np.mean(b_subset[b_indices].mean())\n    b_mean_c_i.append(b_mean)\n\n    w_indices = np.where(w_subset[\"risk_score_t\"].values &gt;= i & w_subset[\"risk_score_t\"].values &lt; i+1)\n    w_mean = np.mean(w_subset[w_indices].mean())\n    w_mean_c_i.append(w_mean)\n\nplt.plot(X = b_percentiles, y = b_mean_c_i)\nplt.show\n\n\n\n\n\nTypeError: ufunc 'bitwise_and' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
  },
  {
    "objectID": "posts/optimal-decision-making/index.html",
    "href": "posts/optimal-decision-making/index.html",
    "title": "Optimal Decision Making",
    "section": "",
    "text": "import pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n# Part B: Explore the Data\n\nmean_income_by_home_ownership = df_train.groupby(\"loan_intent\")[\"person_income\"].mean()\nmean_income_by_home_ownership.plot(kind = \"bar\")\nplt.xlabel('Loan Intent')\nplt.ylabel('Mean Income')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.scatter(df_train[\"loan_amnt\"], df_train[\"loan_percent_income\"], c = df_train[\"loan_status\"])\nplt.xlabel('Loan Amount')\nplt.ylabel('Loan Percent of Income')\nplt.title('Scatter Plot of Loan Amount vs Loan Percent of Income')\nplt.colorbar(label='Loan Status')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nmean_income_by_loan_status = df_train.groupby(\"loan_status\")[\"person_income\"].mean()\nmean_income_by_loan_status\n\nloan_status\n0    70727.744912\n1    48883.176688\nName: person_income, dtype: float64\n\n\n\n# Part C: Build a Model"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html",
    "href": "posts/implementing-logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Optimal Decision Making\n\n\n\n\n\nOptimal Decision Making\n\n\n\n\n\nApr 1, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Decision Making\n\n\n\n\n\nOptimal Decision Making\n\n\n\n\n\nApr 1, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nImplementing logistic regression\n\n\n\n\n\nMar 31, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nImplementing the perceptron alg\n\n\n\n\n\nMar 25, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nThe Women in Data Science (WiDS) Conference at Middlebury College\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Adelie, Gentoo, and Chinstrap Penguins.\n\n\n\n\n\nFeb 23, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]