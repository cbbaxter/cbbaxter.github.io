[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The quick brown fox jumped over the lazy dog."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html",
    "href": "posts/implementing-logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression\nfrom logistic import GradientDescentOptimizer\nfrom matplotlib import pyplot as plt\nimport torch"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#logistic-regression-implementation",
    "href": "posts/implementing-logistic-regression/index.html#logistic-regression-implementation",
    "title": "Implementing Logistic Regression",
    "section": "Logistic Regression Implementation",
    "text": "Logistic Regression Implementation\nHere is the link to my logistic regression implementation"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#abstract",
    "href": "posts/implementing-logistic-regression/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post I implemented the logistic regression algorithm to classify data. It was demonstrated through experimentation that the algorithm will terminate with a loss of zero on linearly seperable data. When the momentum is increased (beta) it will converge at a lower loss more quickly than when the loss is smaller. It was also determined that the logistic regression algorithm is at risk of overfitting, particularly on data with an abundance of features.\n\ntorch.manual_seed(123)\n\ndef classification_data(n_points, noise, p_dims):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n# Code modifies from Prof. Chodrow's \"plot perceptron data\"\ndef plot_logistic_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# Code by Prof. Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n\nX, y = classification_data(300, 0.2, p_dims = 2)\n\nfig, ax = plt.subplots(1, 1, figsize= (4, 4))\nplot_logistic_data(X, y, ax)\n\n\n\n\n\n\n\n\nIn these chuncks of code, written by Professor Chodrow, functions are created for the creation and plotting of data. This plot shows the data generated with minimal noise indicating that it is linearly seperable as we could clearly seperate the two groups by a straight line."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#experiment-1-vanilla-gradient-decsent",
    "href": "posts/implementing-logistic-regression/index.html#experiment-1-vanilla-gradient-decsent",
    "title": "Implementing Logistic Regression",
    "section": "Experiment 1: Vanilla Gradient Decsent",
    "text": "Experiment 1: Vanilla Gradient Decsent\nWith only two dimensions, a sufficiently small learning rate, and a momentum of 0, gradient descent for logistic regression will converge to a weight vector that looks visually correct when plotted as a linear decision boundary between the two clumps of data. To begin this experiment, I generated 300 data points with minimal noise and 2 dimensions. The logistic regression is initialized with a learning rate of 0.1 and a momentum of 0.\n\n## Vanilla gradient descent\ntorch.manual_seed(123)\n\nX, y = classification_data(300, 0.2, p_dims = 2)\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\nalpha = 0.1 # sufficiently small\nbeta = 0\nmax_iter = 1000\ncurr_iter = 0\n\nloss = LR.loss(X, y) \n\nwhile curr_iter &lt; max_iter and loss &gt; 0:\n\n    # not part of the update: just for tracking our progress    \n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\n    opt.step(X, y, alpha, beta)\n    \n    curr_iter += 1\n\n    if curr_iter == max_iter:\n        final_w = torch.clone(LR.w)\n\n\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss\")\n\n\n\n\n\n\n\n\n\nAs seen in this plot, the loss decreases monotonically over the gradient descent iterations.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_logistic_data(X, y, ax)\ndraw_line(final_w, x_min = -2, x_max = 4, ax = ax, color = \"black\")\nax.set_ylim(-2, 4)\n\n\n\n\n\n\n\n\nWhen plotted with the data, we can see that the final weight vector that the logistic regression algorithm converged on weight that corresponds with a correct classification line."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#benefits-of-momentum",
    "href": "posts/implementing-logistic-regression/index.html#benefits-of-momentum",
    "title": "Implementing Logistic Regression",
    "section": "Benefits of Momentum",
    "text": "Benefits of Momentum\nIn order to access the benefits of momentum, we initialize a logistic regression model with the same learning rate but a momentum that has been increased to 0.9.\n\n# Benefits of Momentum\n\ntorch.manual_seed(123)\n\nX, y = classification_data(300, 0.2, p_dims = 2)\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec_mom = []\nalpha = 0.1 # sufficiently small\nbeta = 0.9\nmax_iter = 1000\ncurr_iter = 0\n\nloss = LR.loss(X, y) \n\nwhile curr_iter &lt; max_iter and loss &gt; 0:\n\n    # not part of the update: just for tracking our progress    \n    loss = LR.loss(X, y) \n    loss_vec_mom.append(loss)\n\n    # only this line actually changes the parameter value\n    # The whole definition is: \n    # self.model.w -= lr*self.model.grad(X, y)\n\n    opt.step(X, y, alpha, beta)\n    \n    curr_iter += 1\n\n    if curr_iter == max_iter:\n        final_w = torch.clone(LR.w)\n\n\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\", label = \"Vanilla Gradient Descent\") \nplt.plot(loss_vec_mom,  label = \"Increased Momentum\", color = \"blue\")\nplt.semilogx()\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss\", title = \"The Loss of the Vanilla Gradient Descent and Momentum Gradient Descent over Gradient Descent Iterations\")\n\n\n\n\n\n\n\n\n\n\nIn this plot we can see that the loss has decreased much faster in the gradient descent with the higher momentum than it did during Vanilla Gradient Descent."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#overfitting",
    "href": "posts/implementing-logistic-regression/index.html#overfitting",
    "title": "Implementing Logistic Regression",
    "section": "Overfitting",
    "text": "Overfitting\nIn order to see how susceptible the model is to overfitting, we generate some random synthetic data, with 20 observations and 40 features. While the accuracy on the training data is less than 100%, random test and train data continues to be generated, training a model through gradient descent each time.\nWhen we evaluate the model we see that it achieves an accuracy of 100% on the training data, but only 85% on the testing data. This shows that under certain circumstances, the model can be overfit to the training data, inhibiting its ability to generalize to different data sets.\n\ntorch.manual_seed(123)\n\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nalpha = 0.01 # sufficiently small\nbeta = 0.8\nmax_iter = 1000\n\nn = 20\np = 40\n\ntrain_acc = 0.0\ntest_acc = 0.0\nloss_vec_train = []\nloss = 1.0\n\nwhile train_acc != 1.0:\n    LR.w = None\n    loss_vec1 = []\n    loss_vec2 = []\n    test_acc_vec = []\n    train_acc_vec = []\n    curr_iter = 0\n    X_train, y_train = classification_data(n, 0.3, p_dims = p)\n    X_test, y_test = classification_data(n, 0.3, p_dims = p)\n    loss_tr = LR.loss(X_train, y_train) \n    loss_ts = LR.loss(X_test, y_test)\n\n    while curr_iter &lt; max_iter and loss_tr &gt; 0:\n        loss_tr = LR.loss(X_train, y_train)\n        loss_ts = LR.loss(X_test, y_test)\n        loss_vec1.append(loss_tr)\n        loss_vec2.append(loss_ts)\n        opt.step(X_train, y_train, alpha, beta)\n\n        curr_iter += 1\n\n        preds_train = LR.predict(X_train)\n        train_score = torch.where(preds_train == y_train, 1.0, 0.0).mean()\n        train_acc = train_score.mean()\n    \n        preds_test = LR.predict(X_test)\n        test_score = torch.where(preds_test == y_test, 1.0, 0.0)\n        test_acc = test_score.mean()\n\n        test_acc_vec.append(test_acc)\n        train_acc_vec.append(train_acc)\n\n        if curr_iter == max_iter:\n            final_w_train = torch.clone(LR.w)\n\n    loss_vec_train = loss_vec1\n    loss_vec_test = loss_vec2\n    \n\nprint(\"Training accuracy: \", train_acc)\nprint(\"Testing accuracy: \", test_acc)\n\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\", label=\"training loss\") \nplt.plot(loss_vec_test, color = \"blue\", label=\"testing loss\")\nplt.semilogx()\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss\", title = \"The Loss of the Training and Testing Data over Gradient Descent Iterations\")\n\n## Plot the accuracy\n\n\n\nTraining accuracy:  tensor(1.)\nTesting accuracy:  tensor(0.8500)\n\n\n\n\n\n\n\n\n\nAs seen in this plot the loss of the testing function remains higher than the training loss for most of the iterations and is substancially higher when the maximum number of iterations is reached.\n\nplt.plot(torch.arange(1, len(train_acc_vec)+1), train_acc_vec, label=\"training accuracy\",color = \"black\") \nplt.plot(test_acc_vec, color = \"blue\", label=\"testing accuracy\")\nplt.semilogx()\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Accuracy\", title = \"The Accuracy of the Training and Testing Data over Gradient Descent Iterations\")\n\n\n\n\n\n\n\n\nThe evidence of overfitting is more clearly visible in this graph where we can see the accuracy of the classification based on the weight vector for each iteration. As we can see, the training accuracy is quite a bit higher than the accuracy of the model on the training data, suggesting that the model was overfit on the training data specifications."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#discussion",
    "href": "posts/implementing-logistic-regression/index.html#discussion",
    "title": "Implementing Logistic Regression",
    "section": "Discussion",
    "text": "Discussion\nThrough experimenting with different implementations of Logistic Regression we have been able to investigate how the model works. We determined that by increasing the momentum, the rate at which the gradient descent converged on a lower loss increased. We also showed that this logistic regression model is susceptible to overfitting by examining how it worked on data with a large number of features and observing that its performance was better and more accurate on the training data as opposed to the testing set."
  },
  {
    "objectID": "posts/voice-emotion-classification/index.html",
    "href": "posts/voice-emotion-classification/index.html",
    "title": "Final Project Blog Post",
    "section": "",
    "text": "The first seven sections of this blog post were completed collaboratively with my partner Otis.\nThe final personal reflection section was completed individually."
  },
  {
    "objectID": "posts/voice-emotion-classification/index.html#results",
    "href": "posts/voice-emotion-classification/index.html#results",
    "title": "Final Project Blog Post",
    "section": "Results",
    "text": "Results\nAfter running our model for 50 epochs, we found that the model performed with a 39.9% validation accuracy which is significantly higher than our base rate of 16.6%. In terms of our testing data set of size 201 audio files, this means that the base rate would correctly predict around 30 audio files while our model would predict around 72 audio files correctly. This is about a 42% increase in the number of correctly predicted audio files.\nIn observing the confusion matrix of the classification results from the final epoch, we are able to visualize how the model’s predictions compared to the actual emotion values.\nFigure 1: Confusion Matrix Comparing the 75th Epoch’s Model Predictions with the Actual Emotion Classifications of our Validation Data Table 1: \nAs seen in Figure 1, the model was much more likely to classify an audio file as certain emotions than others. The model underpredicted emotions such as Fearful which made up 12.5% of the test data, but was only predicted 6.60% of the time, and overpredicted others such as Happy which only makes up 12.8% of the test data, but was predicted 21.2% of the time.\nAn interesting observation demonstrated in the confusion matrix is some of the misclassifications. The model misclassified Neutral audio files as Calm most often, which are emotions that many people might consider fall into the same domain. Similarly, emotions like ‘Angry’, ‘Disgust’, and ‘Happy’, which are all elevated responses, were more likely to be misclassified as one another.\nTable 1: \nDespite the model successfully improving the base rate, we believe that the model is underperforming because of overfitting. Our training accuracy was 92.2% which is significantly higher than our validation accuracy of 39.9%. Ideally the training accuracy and the validation accuracy would be much more similar.\n\nConcluding Discussion\nOur primary goal for this project was to create a model that correctly classifies the emotional tone of an audio file with an accuracy greater than randomly assigning categories, which we achieved. Our model doubled the accuracy over the baseline and demonstrated its effectiveness at classifying emotions. We created a Jupyter notebook, linked at the beginning of this blog post, where we were able to vectorize and visualize the audio files. We were able to gain a strong understanding of audio analysis and better comprehend how our neural network behaves.\nIn comparison to the contemporary literature on this subject, our model did not perform with the same level of accuracy when classifying emotions. Many of their models performed with a classification accuracy between 60-80%, whereas ours was approximately 40%. Given that we were performing the earlier stages of experimentation, we feel that our model performed quite well comparatively. Due to time and computational power constraints we were also only able to perform 50 epochs and had to keep the layers of our CNN rather small. If we had more time to improve our project we would work to extract features from the mel spectrogram in order to ideally improve the effectiveness of our CNN. Zhang et. al. write that the input to CCNs should be simple, as it is challenging for CNNs to extract all of the features of the Mel Spectrogram at once (Zhang et al. (2021)).\n\n\nGroup Contributions Statement\nThe majority of work on this project was done collaboratively by peer programming or live sharing on VSCode. We occasionally did work apart but this was mostly isolated to research and running the model with different parameters. Overall, we feel pretty comfortable in the breakdown of the work in this project.\n\n\nPersonal Reflection\nI learned a lot over the course of this project. Firstly, I feel that I greatly improved my programming skills by learning how to analyze auditory data and implementing a Convolutional Neural Network. I also felt that I developed my ability to both manage and communicate about such a large project. When we were starting out, the scope of the project felt very large and it was a little difficult to figure out what we were supposed to be doing or starting with. I think that allowed me to develop my planning and project management skills as we had to take some time to map out what we were going to work on. We also were in almost constant communication, meeting several times a week to work on the project together and would text one another to keep them updated if we did any work on our own.\nI feel pretty good about what we achieved, as out primary goal was to build a model that outperformed the base rate, which it did! That being said, our accuracy was only 40% so had we had more time I would have wished to improve upon that, as our model still gets more wrong than it gets right. In terms of my other goals, I was really happy with the project that we selected because I found it challenging, but also very interesting and fulfilling.\nI think that one thing that will stick with me from working on this project was collaborating with Otis and learning how to successfully develop as a team. I also feel that my experience in researching contemporary studies and using that to inform the decisions I make about how to handle data and my model decisions. I feel more comfortable tackling machine learning projects and think that in future I will be more prepared when doing larger scale projects or working with data that I am unfamiliar with."
  },
  {
    "objectID": "posts/optimal-decision-making/index.html",
    "href": "posts/optimal-decision-making/index.html",
    "title": "Optimal Decision Making",
    "section": "",
    "text": "import pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n# Part B: Explore the Data\n\nmean_income_by_home_ownership = df_train.groupby(\"loan_intent\")[\"person_income\"].mean()\nmean_income_by_home_ownership.plot(kind = \"bar\")\nplt.xlabel('Loan Intent')\nplt.ylabel('Mean Income')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.scatter(df_train[\"loan_amnt\"], df_train[\"loan_percent_income\"], c = df_train[\"loan_status\"])\nplt.xlabel('Loan Amount')\nplt.ylabel('Loan Percent of Income')\nplt.title('Scatter Plot of Loan Amount vs Loan Percent of Income')\nplt.colorbar(label='Loan Status')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nmean_income_by_loan_status = df_train.groupby(\"loan_status\")[\"person_income\"].mean()\nmean_income_by_loan_status\n\nloan_status\n0    70727.744912\n1    48883.176688\nName: person_income, dtype: float64\n\n\n\n# Part C: Build a Model"
  },
  {
    "objectID": "posts/newtons-method/index.html",
    "href": "posts/newtons-method/index.html",
    "title": "Newton’s Method",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression\nfrom logistic import NewtonOptimizer\nfrom logistic import GradientDescentOptimizer\nfrom matplotlib import pyplot as plt\nimport torch\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/newtons-method/index.html#abstract",
    "href": "posts/newtons-method/index.html#abstract",
    "title": "Newton’s Method",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post I used an implementation of Newton’s Method to optimize my previously constructed Logistic Regression model. It was demonstrated through experimentation that a Logistic Regression model using Newton’s Method will converge to a weight vector that allows for the proper descision boundary for linearly seperable data, and that it can do it in fewer iterations than standard gradient descent. However when the learning rate is too large, it will fail to converge. I then analyzed the computational cost of Newton’s method compared to standard gradient descent and determined that with a large number of features, the computational expense of Newton’s Method might make it a less desirable optimizer as opposed to standard gradient descent, despite its speed. It was determined that when the learning rate is increased too much, the model fails to converge."
  },
  {
    "objectID": "posts/newtons-method/index.html#part-a-implement-newtons-method",
    "href": "posts/newtons-method/index.html#part-a-implement-newtons-method",
    "title": "Newton’s Method",
    "section": "Part A: Implement Newton’s Method",
    "text": "Part A: Implement Newton’s Method\nHere is my implementation of Newton’s Method with Newton’s Method.\nIn the following code chunk, I include functions by Professor Phil Chodrow that I use throughout the blog post to initialize and plot my data.\n\nimport torch\ntorch.manual_seed(123)\n\ndef classification_data(n_points, noise, p_dims):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n# Code modifies from Prof. Chodrow's \"plot perceptron data\"\ndef plot_logistic_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# Code by Prof. Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)"
  },
  {
    "objectID": "posts/newtons-method/index.html#part-b-perform-experiments",
    "href": "posts/newtons-method/index.html#part-b-perform-experiments",
    "title": "Newton’s Method",
    "section": "Part B: Perform Experiments",
    "text": "Part B: Perform Experiments\n\n1. Chosing an appropriate value of $$\nTo begin this experiment, we first initialize 500 random data points and initialized the Logistic Regression model with an \\(\\alpha\\) of 10. I ran the training loop for 1000 iteration (or until the loss converged at zero).\n\ntorch.manual_seed(123)\n\nX, y = classification_data(500, 0.5, p_dims = 2)\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\nalpha = 10 # sufficiently small\nmax_iter = 1000\ncurr_iter = 0\n\nloss = LR.loss(X, y) \n\nwhile curr_iter &lt; max_iter and loss &gt; 0:\n\n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\n    opt.step(X, y, alpha)\n    \n    curr_iter += 1\n\n    final_w = torch.clone(LR.w)\n\n\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss\", title=\"Observing the Loss over iterations using Newton's Method\")\n\n\n\n\n\n\n\n\n\nBy visualizing the loss vector over the iterations we can see that the loss converges at a value of approximately 0.2 after about 100 iterations.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_logistic_data(X, y, ax)\ndraw_line(final_w, x_min = -2, x_max = 4, ax = ax, color = \"black\")\nax.set_ylim(-2, 4)\n\n\n\n\n\n\n\n\nWhen we then plot the values of w as the decision boundary we observe that it seems to split the data into the two intended sections.\n\n\n2. Experimenting with the Rate of Convergence\nTo compare the rate of convergence between Newton’s Method and Standard Gradient Descent, I ran my logistic regression model again on the same data this time using SGD as the optimization step.\n\n\ntorch.manual_seed(123)\n\nLR = LogisticRegression()\nopt_standard_grad = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec_grad = []\nalpha = 0.1 \nbeta = 0.0\nmax_iter = 2000\ncurr_iter = 0\n\nloss = LR.loss(X, y) \n\nwhile curr_iter &lt; max_iter and loss &gt; 0:\n\n    loss = LR.loss(X, y) \n    loss_vec_grad.append(loss)\n\n    opt_standard_grad.step(X, y, alpha, beta)\n    \n    curr_iter += 1\n\n    if curr_iter == max_iter:\n        final_w = torch.clone(LR.w)\n\n\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\", label = \"Newton's Method\") \nplt.plot(loss_vec_grad,  label = \"Standard Gradient Descent\", color = \"blue\")\nplt.semilogx()\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss\", title = \"Comparing the Loss Over Iterations for Newton's Method and Standard Gradient Descent\")\n\n\n\n\n\n\n\n\nI then plotted the evolution of the loss of both models across the iterations. As evident in this plot, Newton’s method converges much more quickly than standard gradient descent.\n\n\n3. If $$ is too large, Newton’s method fails to converge\nIn order to test what would happen with a very large learning rate, I again used Logistic Regression on the same data, this time with an alpha of 1025.\n\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\nloss_vec_large_a = []\nalpha = 1025\nmax_iter = 1000\ncurr_iter = 0\n\nloss = LR.loss(X, y) \n\nwhile curr_iter &lt; max_iter and loss &gt; 0:\n\n    loss = LR.loss(X, y) \n    loss_vec_large_a.append(loss)\n\n    opt.step(X, y, alpha)\n    \n    curr_iter += 1\n\n    if curr_iter == max_iter:\n        final_w = torch.clone(LR.w)\n\n\nplt.plot(torch.arange(1, len(loss_vec_large_a)+1), loss_vec_large_a, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of newtons iterations\", ylabel = \"Loss\", title=\"Observing the Loss over iterations using Newton's Method\")\n\nloss_vec_large_a[-5:]\n\n\n\n\n\n\n\n\n\nAs shown in this graph and the output of the loss vector, before the loss value becomes NaN, it begins to increase indicating that Newton’s Method is not behaving as it should with such a large learning rate."
  },
  {
    "objectID": "posts/newtons-method/index.html#part-c-operation-counting",
    "href": "posts/newtons-method/index.html#part-c-operation-counting",
    "title": "Newton’s Method",
    "section": "Part C: Operation Counting",
    "text": "Part C: Operation Counting\nSuppose that, given a computation unit \\(c\\) and a matrix of \\(p \\times p\\):\n\nIt costs \\(c\\) to compute the loss, \\(L\\)\nIt costs \\(2c\\) to compute the gradient \\(\\nabla L\\)\nIt costs \\(pc\\) o compute the Hessian\nIt costs \\(k_1 p^\\gamma\\) to invert a \\(p \\times p\\) matrix\nIt costs \\(k_2 p^2\\) to perform the matrix vector multiplication required by Newton’s method\nNewton’s method converges to an adequate solution in \\(t_\\mathrm{nm}\\)\nGradient Descent converges to an adequate solution in \\(t_\\mathrm{gd}\\) steps\n\nWe are able to write expressions describing the total computation costs of Newton’s method as compared to gradient descent.\n\\[ \\begin{aligned} O(\\text{Gradient Descent}) &= L + t_\\mathrm{gd} \\times \\nabla L \\\\ &=  c + t_\\mathrm{gd} \\times 2c  \\end{aligned}\\]\n\\[ \\begin{aligned} O(\\text{Newton's Method}) &= L + t_\\mathrm{nm} \\times (\\nabla L + \\text{Hessian} + \\text{invert} + \\text{Matrix-vector Multiplication})\n\\\\ &= c + t_\\mathrm{nm} \\times (2c + pc + k_1 p^\\gamma + k_2 p^2) \\end{aligned}\\]\nIn order to determine how any fewer iterations of Newton’s method would require less computational power, I compared these two equations.\n\\[ \\begin{aligned}\nO(\\text{Gradient Descent}) &&gt; O(\\text{Newton's Method}) \\\\\nc + 2t_{gd}c &&gt; c + t_{nm}*(2c + pc + k_1p^\\gamma + k_2p^2) \\\\\n2t_{gd}c &&gt; t_{nm}*(2c + pc + k_1p^\\gamma + k_2p^2) \\\\\nt_{gd} &&gt; t_{nm}*(1 + \\frac{p}{2} + \\frac{k_1p^\\gamma}{2c} + \\frac{k_2p^2}{2c})\n\\end{aligned} \\]\nAs demonstrated in this inequality, in order for Newton’s method to be less computationally expensive than standard gradient descent, \\(t_{nm}\\) must be smaller than \\(t_{gd}\\) by a factor of \\((1 + \\frac{p}{2} + \\frac{k_1p^\\gamma}{2c} + \\frac{k_2p^2}{2c})\\).\nAssuming that p is a very large value we can simplify this equation to the following As $ 2 &lt; 3$.\n\\[ t_{gd} &gt; t_{nm} \\times O(p^\\gamma) \\]\nThis means that when \\(p\\) is very large it greatly increases the computational expense of Newton’s Method, and its speed might not be worth its complexity as opposed to standard gradient descent."
  },
  {
    "objectID": "posts/newtons-method/index.html#conclusion",
    "href": "posts/newtons-method/index.html#conclusion",
    "title": "Newton’s Method",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post I was able to implement Newton’s Method as an optimizer for my implementation of Logistic Regression. Through experimentation, I demonstrated that it was able to converge faster than standard gradient descent, but later showed that when the number of features is very large it greatly increases the computational expense. I also demonstrated that with very high learning rates, the model begins to diverge. Through this investigation I was able to access the benefits and risks of Newton’s Method for optimization."
  },
  {
    "objectID": "posts/women-in-data-science/index.html",
    "href": "posts/women-in-data-science/index.html",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "Abstract\nOn March 4, 2024 I attended Middlebury College’s Women in Data Science Conference that aimed to highlight the work of women using data across different disciplines. Part A of this blogpost uses Corbett and Hill’s report “Solving the Equation: The Variables for Women’s Sucess in Engineering and Computing” to discuss the issues facing female representation in the fields of Engineering and Computing and how events that spotlight women can have a hugely positive effect on breaking down biases and building community. In Part B, I reflect on the conference itself, discsussing the different lightning presentations by Middlebury professors and the keynote speaker, Sarah Brown.\n\nPart A: Why Spotlight Women in Data Science?\nWhy is it a problem that women are underrepresented in computing, math, and engineering? For whom is it a problem?\nThe underrepresentation of women in computing, math, and engineering is not only a problem for many women in those fields, but also contributes to larger societal injustices and problems. Diversity in the workforce not only contributes to creativity, productivity, and innovation, but by excluding approximately half of the population from engineering and technical decisions societal inequalities are often perpetuated. This winter, I began reading a book called Invisible Women: Data Bias in a World Designed for Men by Caroline Criado-Perez that discussed the disparity in the data that has led to many of accepted societal designs and standards. In it, they include a quote from Simone de Beauvoir’s The Second Sex, that states “Representation of the world, like the world itself, is the work of men; they describe it from their own point of view, which they confuse with the absolute truth”. This idea is highlighted throughout Corbett and Hill’s discussion of the negative impact of the lack of representation of women in engineering and computing fields in their report “Solving the Equation: The Variables for Women’s Success in Engineering and Computing”. The lack of representation also makes it more difficult for women who are entering the field. With a lack of role models or representation in high power positions, it can be difficult for women to feel confident or like they belong in computing or engineering fields. This in-turn continues to propagate and negatively reinforce the issues surrounding the underrepresentation in the first place.\nHow is the representation and status of women in computing today different from the 1950s and 1960s? What are some of the forces that brought this change?\nSince the 1950s and 60s women have made substantial advances in many STEM fields, making up 39% of chemists in 2013 as opposed to 8% in 1960. In Computing fields however, women make up just over a quarter of the workforce which is approximately the same as it was in 1960. Only 12% of engineers are women, which is a substantial increase from the 1% in 1960, yet still remains a strikingly small percentage. An important way to facilitate the increase of female representation is by changing the environment. Corbett and Hill discuss how Harvey Mudd College was able to increase the percentage of women graduating from its computing program from 12% to 40% in 5 years by making three changes that made their courses a more welcoming and accessible environment for female computer scientists. Recognizing that the problem not only lies with attracting women to a program, but also keeping them invested and involved, is another essential element of representation.\nWhich of the barriers and unequal challenges described in the section “Why So Few” can be eroded by events that spotlight the achievements of women in STEM?\nA large barrier for women in Computing fields is the lack of a perceived community, both in their role, which is stereotypically often perceived as very solitary, and as an underrepresented group of people. Women are more likely than men to express an interest in work with a clear social purpose and as Corbett and Hill write, “Incorporating communal aspects - both in messaging and substance - into engineering and computing work will likely increase the appeal of these fields to communally oriented people, many of whom are women.” Events that highlight the success of women also help cultivate a sense of belonging which is essential for a field or workplace when trying to attract and retain diversity. Through events that spotlight the achievements of women, we are able to also help erode implicit gender biases and stereotypes. Corbett and Hill discuss how exposing students to female role models early in life might be able to change the implicit biases between math capabilities and gender.\n\n\nPart B: Middlebury Women in Data Science Conference\nMiddlebury professor Amy Yuen presented on ‘Is the UN Security Council a Democratic Institution?’ and discussed how she has used data in her research on the UN’s security council and how the status of different members effect the resolutions and types of meetings the council holds. The UN Security council is comprised of 15 members, with 5 permanent members who hold veto power, and 10 elected members that serve 2-year terms. In measuring the different resolutions and meetings brought forward by the permanent members versus the elected members, Professor Yuen looked to access how the different elected members affect the council. They found differences in sponsorship of resolutions, determining that permanent members often put out the most resolutions that relate to current events while elected members focus on more thematic issues such as climate change. They found that the actual line of best fit of their data fit closer to their modelled ‘most inclusive cycle’ which suggests that while the Security Council is definitely not democratic in the way we often think of it, it is fairly representative of the world scene and engages more states than people would imagine. I learned a lot about how the UN Security Council functions from this presentation and also a fun example of how data is used outside of Computer Science research. One thing that I found interesting was the challenges of aquiring data. Professor Yuen discussed how even though all of this data is public record and has been well-documented, the data provided by the UN was at times incorrect and had to be re-checked by her research assistants.\nGeography professor Dr. Jessica L’Roe presented on her research on deforestation. She showed us several of her projects, and discussed how she and her research assitance both collected data and analyzed it. One of the first examples she discussed was in Brazil, where while the government was very good at detecting deforestation, they found it difficult to actually identify who was responsible for clearing land. The government created a new initiative that had people register their land, but the self-reports provided by people were often incorrect as people would lie about the size of their land. So while they have a lot of data from aerial observation and statistics, a lot of their research relies on interviewing other people. She echoed this in another example of her research in Uganda. I found her discussion of how interviewing of locals provided them with essential information in wildlife and woodlots conservation. I think that sometimes in Computer Science and Statistics data can feel really impersonal and it was nice to bring the discussion back to the people who are the basis of and are greatly affected by models.\nFinally, Professor Laura Biester presented on her research using Computation Linguistic Models to study Mental Health issues. Research into mental health involves the fundamental challenge of access to quality data due to the privacy protections in place for health data, so many researchers rely on self-reports where an individual has posted on the internet about their diagnosis. Typically, models built on self-report datasets do not generalize as well, so Professor Biester and her colleagues turned to Reddit. Looking at over 360 JSON files with all of the Redditposts and comments from 2006-2019, they developed a dataset with 20k diagnosed users and controls then used Bidirectional Encoder Representations from Transformers (BERT) to extract contextual representations of words that were associated with the lead-up to a depression diagnosis. I learned a lot about how these large language model studies are conducted through Professor Biester’s presentation. I found her discussion on the limitations of Reddit data as it includes very little demographic information about the individuals to be interesting, and made me wonder at how the results might vary if we had information about the gender of the user. How would the terminology vary for a female identifying, or non-binary individual? I found Professor Biester’s discussion on how she uses huge amounts of data to investigate language to be very interesting and informative.\nProfessor Sarah Brown, from the University of Rhode Island, was the main keynote speaker of the event. She presented on ‘Data Science Skills in Unexpected Places’ and discussed different non-technical skills that she has aquired over the years that help her solve data science problems and overcome roadblocks when faced with an issue. One of her main points that I found particularly interesting was her discussion of data as a primary source that needs to be assessed within its context in order to be properly understood. She began her presentation by showing a three circled-venn diagram that had ‘Computer Science’, ‘Domain Experts’, and ‘Statistics’ in each circle, with the overlapping section highlighted as ‘Data Science’. She discussed how each discipline is not simply a topic, but rather a community full of people, norms, and vocabularies that need to be understood in order to properly analyze its data.\nProfessor Brown also discussed the question of accuracy versus fairness, suggesting a change from the traditional data pipeline to incorporate the question of fairness into the earlier stages when one is exploring the data and variables:\ncollect -&gt; clean -&gt; explore -&gt; fair? -&gt; model -&gt; deploy\nShe discussed an example in healthcare where by changing the target variable of the model, they were able to make it more equitable. Computer Science is a novice field with disproportionate power, which can lead to chaos. By incorporating discussions of fairness and auditing our models into our processes we are able to work on addressing bias in models as we are going instead of realizing after deployment that it is behaving problematically. One thing that I found particularily impactful was when Professor Brown stated how we can have an incredible accurate algorithm that is biased, because the world is biased and therefore the data generated by the world is biased.\nReflection\nAs I was scrolling through Instagram the other day I came across a commercial that was made by Mercedes-Benz for International Women’s Day in 2023. The video is narrated by a few young girls and the overall premise is that they are not interested in being ‘exceptional’ because they are the first woman to do something, they would rather be one of many. I think that messaging really highlights what I have been reflecting on throughout this blogpost - how the underrepresentation of women in Computing and Engineering fields can feel like a self-perpetuating cycle, and that by highlighting and empowering successful women in these fields we are able to build representation and community. I had a great time at Middlebury’s Women in Data Science conference and felt that I both learned a lot, and also got to meet a lot of other students through their ‘Get to know you bingo activity’ and left happily with a midd data tshirt."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n# Load Data\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\nAbstract\nIn this blog post I investigated the classification of Gentoo, Adelie, and Chinstrap penguins. It was determined that using Logistic Regression with the variables culmen length, culmen depth, and the island where the penguins are found allowed for the correct classification of penguin 100.0% of the time on the test data. The culmen is the dorsal ridge atop the bill of the penguin, and its depth and length varies between the three species.\n\nFigure 1: Adelie penguins\nimage source: https://i.natgeofe.com/n/00c35612-9827-40d9-a04e-6079806dca52/3798855_2x1.jpg\n\nFigure 2: Chinstrap penguin\nimage source: https://live.staticflickr.com/707/31973598931_6d9bfd3f18_b.jpg\n\nFigure 3: Gentoo penguin\nimage source: https://cdn.britannica.com/08/152708-050-23B255B3/Chinstrap-penguin.jpg\nLogistic regression was compared with Decision Tree Classification and Support Vector Classification and was determined to be the most accurate model when the C value was set to 17.\n\n# Data Preparation\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nsummary_table = train.groupby([\"Species\", \"Island\"])[\"Body Mass (g)\"].mean()\nprint(summary_table)\n\nSpecies                                    Island   \nAdelie Penguin (Pygoscelis adeliae)        Biscoe       3711.363636\n                                           Dream        3728.888889\n                                           Torgersen    3712.804878\nChinstrap penguin (Pygoscelis antarctica)  Dream        3743.421053\nGentoo penguin (Pygoscelis papua)          Biscoe       5039.948454\nName: Body Mass (g), dtype: float64\n\n\nFrom this summary table it is evident that Chinstrap penguins only live on the Dream island while Gentoo penguins only live on Biscoe. Meanwhile, Adelie Penguins live on Biscoe, Dream, adn Torgersen. This suggests that the Island where an individual is located may be a good variable for a machine learning algorithm as we know that if an individual is located on torgersen they must be an Adelie and that if an individual is found on Dream it is not a Gentoo penguin.\n\nmass_table = sns.relplot(data = train, x = \"Body Mass (g)\" , y = \"Flipper Length (mm)\", hue = \"Species\")\nmass_table.set_axis_labels(\"Body Mass (g)\", \"Flipper Length (mm)\")\nmass_table.add_legend(frameon=True)\nmass_table.legend.set_bbox_to_anchor((1.5, .6))\nplt.gca().set(title = \"Comparing the Body Mass and Flipper Length of Penguins\")\nprint(mass_table)\n\n&lt;seaborn.axisgrid.FacetGrid object at 0x12f8a37f0&gt;\n\n\n\n\n\n\n\n\n\nFrom this figure it is apparent that the flipper length of a penguin is positively related with their body mass. It is clear that Gentoo penguins are generally larger than chinstrap and adelie penguins, and the latter two are relatively similar in size.\n\nflipper_table = sns.relplot(data = train, x = \"Flipper Length (mm)\" , y = \"Culmen Length (mm)\", hue = \"Species\", style = \"Island\")\nflipper_table.set_axis_labels(\"Flipper Length (mm)\", \"Culmen Length (mm)\")\nflipper_table.add_legend(frameon=True)\nplt.gca().set(title = \"Comparing the Culmen Length and Flipper Length of Penguins\")\nflipper_table.legend.set_bbox_to_anchor((1.5, .6))\nprint(flipper_table)\n\n&lt;seaborn.axisgrid.FacetGrid object at 0x154b5fac0&gt;\n\n\n\n\n\n\n\n\n\nIn this plot we can see that while Adelie and Chinstrap penguins have similar Flipper lengths, Chinstrap penguins generally have longer culmens. The flipper and culmen length of Adelie penguins does not appear to vary greatly depending on the island on which they were found.\n\nculmen_table = sns.relplot(data = train, x = \"Culmen Depth (mm)\" , y = \"Culmen Length (mm)\", hue = \"Species\", style = \"Island\")\nculmen_table.set_axis_labels(\"Culmen Depth (mm)\", \"Culmen Length (mm)\")\nculmen_table.add_legend(frameon=True)\nplt.gca().set(title = \"Comparing the Culmen Length and Culmen Depth of Penguins\")\nculmen_table.legend.set_bbox_to_anchor((1.5, .6))\nprint(culmen_table)\n\n&lt;seaborn.axisgrid.FacetGrid object at 0x154959040&gt;\n\n\n\n\n\n\n\n\n\nIn this plot, we observe that the culmen length and culmen depth seem to be generally distinct between species. Through this preliminary analysis, I decided to focus my search on looking at the combination of qualitative variables with the quantitative variables that relate to the size of the penguins, as they appear to vary in body mass, flipper length, and the size of their culmens.\n\nfrom itertools import combinations\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\ndef logisticRegression(X_train, y_train):\n  best_score = 0\n  best_c = 0\n  for c in range(1, 20): \n    LR = LogisticRegression(max_iter = 5000, C = float(c))\n    LR.fit(X_train, y_train)\n    cv_scores_LR = cross_val_score(LR, X_train, y_train, cv = 5)\n    if cv_scores_LR.mean() &gt; best_score:\n      best_score = cv_scores_LR.mean()\n      best_c = c\n  return cv_scores_LR.mean(), best_c\n\ndef decisionTree(X_train, y_train):\n  best_score = 0\n  best_max_depth = 0\n  for i in range(1, 100):\n    DT = DecisionTreeClassifier(max_depth = i)\n    DT.fit(X_train, y_train)\n    cv_scores_DT = cross_val_score(DT, X_train, y_train, cv = 5)\n    if cv_scores_DT.mean() &gt; best_score:\n      best_score = cv_scores_DT.mean()\n      best_max_depth = i\n  return best_score, best_max_depth\n\ndef svc(X_train, y_train):\n  xscaler = preprocessing.StandardScaler().fit(X_train)\n  X_scaled = xscaler.transform(X_train)\n  gammas = 10**np.arange(-5, 5)\n  best_score = 0\n  best_gamma = gammas[0]\n  for gamma in gammas:\n    svc = SVC(gamma = gamma)\n    svc.fit(X_scaled, y_train)\n    cv_scores_svc = cross_val_score(svc, X_scaled, y_train, cv = 5)\n    if cv_scores_svc.mean() &gt; best_score:\n      best_score = cv_scores_svc.mean()\n      best_gamma = gamma\n  return best_score, best_gamma\n\nbest_vars = []\nmax_score = 0\nmodelType = \"\"\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    select_X_train = X_train[cols]\n    \n    # -- Logistic Regression --\n    LR_score, c = logisticRegression(select_X_train, y_train)\n    if LR_score &gt; max_score:\n      max_score = LR_score\n      best_vars = cols\n      modelType = \"LR\"\n      print(max_score, best_vars, modelType, c)\n\n    # -- Decision Tree -- \n    DT_score, max_depth = decisionTree(select_X_train, y_train)\n    if DT_score &gt; max_score:\n      max_score = DT_score\n      best_vars = cols\n      modelType = \"DT\"\n      print(max_score, best_vars, modelType, max_depth)\n   \n    # -- SVC --\n    svc_score, gamma = decisionTree(select_X_train, y_train)\n    if svc_score &gt; max_score:\n      max_score = svc_score\n      best_vars = cols\n      modelType = \"svc\"\n      print(max_score, best_vars, modelType, gamma)\n\nprint(best_vars, max_score, modelType) \n\n\n0.9530920060331824 ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)'] LR 1\n0.9883107088989442 ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] LR 1\n0.9883861236802414 ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)'] LR 18\n1.0 ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] LR 17\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 1.0 LR\n\n\nSimilar to my observations during my initial investigation into the different variables, we see that Island, Culment Length, and Culmen Depth were used in the most accurate model. The Linear Regression model outperformed both the SVC and the Decision Tree. It was determined that when C = 17, the model was 100% accurate. The C is the inverse regularization strength, and the smaller the value specifies stronger regularization (sklearn documentation). C being 17 indicates a weaker regularization and suggests that the model may be focusing on minimizing error in the training data and may not regularize as well. I decided to test the model again with more cross validation.\n\nX_select = X_train[['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']]\nLR = LogisticRegression(max_iter = 5000, C = 1.0)\nLR.fit(X_select, y_train)\ndefault_c_cv_scores_LR = cross_val_score(LR, X_select, y_train, cv = 5)\ndefault_c_cv_scores_LR.mean()\n\nLR = LogisticRegression(max_iter = 5000, C = 17.0)\nLR.fit(X_select, y_train)\ncv_scores_LR = cross_val_score(LR, X_select, y_train, cv = 5)\ncv_scores_LR.mean()\n\nprint(default_c_cv_scores_LR.mean())\nprint(cv_scores_LR.mean())\n\n0.9843891402714933\n1.0\n\n\nWhen I re-ran the Logistic Regression Model with cross validation with the default value of C vs the one that was found to be most effecient in the above function, it is clear that the Logistic Regression model is already fairly accurate at classifying types of penguins using Culmen Depth, Culmen Length, and the Island that they are found.So I decided to test the LR model on the test data.\n\n# Testing the Model on Test Data\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ncols = ['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nX_test, y_test = prepare_data(test)\n\nX_select = X_train[['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']]\nLR = LogisticRegression(max_iter = 5000, C = 1.0)\nLR.fit(X_select, y_train)\n\ny_preds = LR.predict(X_test[cols])\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\n\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_preds, normalize = \"true\")\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\nAs shown in the above confusion matrix the model accurately predicted the species of penguin each time when looking at the Island, the Culmen Length, and Culmen Depth.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\n\nDiscussion\nIt was determined that when classifying Chinstrap, Gentoo, and Adelie penguins, looking at the length and depth of their culmen along with the island where they are found. The model was able to correctly classify the type of penguin 100% of the time. Through a preliminary investigation of the data, it was evident that Island would likely be a helpful variable in determining the species of the penguins. This was supported when Island was selected as one of the variables in the three variable combination for the best accuracy in the for loop. In the confusion matrix, it is evident that all three types of penguins were correctly identified in the test dataset.\nIn this blog post I learned the importance of investigating different values of parameters and how they affect the model. When I was looping through the different variables, I had the script print out the combinations of variables, their accuracy, and the type of model so I could see how it ultimately found the best one. It was interesting to see how changing the value of C in Logisitic Regression could change the model’s ability to accurately classify. For example moving C = 16 to C = 17, boosted the accuracy from 99.2% to 100.0%"
  },
  {
    "objectID": "posts/implementing-the-perceptron-alg/index.html",
    "href": "posts/implementing-the-perceptron-alg/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/implementing-the-perceptron-alg/index.html#abstract",
    "href": "posts/implementing-the-perceptron-alg/index.html#abstract",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post I implemented the perceptron alogorithm to classify linearly seperable data. It was demonstrated through experimentation that the algorithm will terminate with a loss of zero on linearly seperable data, but will never reach a loss of zero and either not terminate, or reach a maximum number of iterations on non-linearly seperable data. The time complexity of the algorithm is dependent on the number of data points and features."
  },
  {
    "objectID": "posts/implementing-the-perceptron-alg/index.html#perceptron-implementation",
    "href": "posts/implementing-the-perceptron-alg/index.html#perceptron-implementation",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Perceptron Implementation",
    "text": "Perceptron Implementation\nHere is the link to my perceptron implementation.\nHere is my implementation of the perceptron.grad() function:\ndef grad(self, X, y):\n    score = self.score(X)\n    return torch.where((y * score) &lt; 0, y @ X, 0.0)   \nThe gradient function begins by calculating the score of model, using the following formula to calculate the dot product between the vector \\(w\\) and \\(X_i\\)\n\\[s_i = \\langle w, X_i\\rangle\\]\nThe gradient is then calculated by the following formula:\n\\[{1}\\left[s_i y_i &lt; 0 \\right] y_i \\ x_i\\]\nIn my implementation I used torch.where() to replicate the indicator function (fancy \\(1\\)). The first parameter of the torch.where() function checks the condition that the score muliplied by y is greater than zero. If it is, the function returns the dot product of \\(y\\) and \\(X\\), and if not it returns 0.\n\n# Generate Data\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2 * y - 1\n    y = y.type(torch.FloatTensor)\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\nfig, ax = plt.subplots(1, 1, figsize= (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nThis code, by Professor Phil Chodrow, generates linearly seperable data which is neccessary to test my impletation of the perceptron algorithm. The plot shows the data, which we can clearly see is clumped in two different categories that could be seperated by a straight line.\n\nPart A\n\n# Check your Implementation \ntorch.manual_seed(1234)\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter = 1000\n\nwhile loss &gt; 0 and max_iter &gt; 0:\n    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    max_iter = max_iter - 1\n\nIn this code block, we use perceptron on the generated linearly seperable data to test its implementation. A while loop runs while the loss value is greater than one, or until the maximum number of iterations is reached. The loss is calculated at the beginning of each iteration. Then, a random data point is selected and a perceptron update is performed using the random data point. A perceptron step once again calculates the loss using the randomly selected data point, then uses Perceptron’s grad() function to determine the change to the vector \\(w\\). Once the loss reaches 0 or the maximum number of iterations is reached, the loop will terminate.\n\nplt.plot(loss_vec, color = \"skyblue\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"skyblue\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Loss over Perceptron Interations for Linearly-Seperable Data\")\n\n\n\n\n\n\n\n\nThis plot, showing the loss of the function at different iterations demonstrates the efficacy of my implementation of the perceptron algorithm since it terminates after approximately 130 iterations. This means that the algorithm is able to determine a linear boundary that allows for 0 loss before the algorithm reaches 1000 iterations.\n\n\nPart B : Experiments\n\n# Generate Data Functions\n# by Prof. Phil Chodrow\n\ndef perceptron_data(n_points, noise, p_dims):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2 * y - 1\n    y = y.type(torch.FloatTensor)\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\n\n# Generate 2D Linearly Seperable Data\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims = 2)\n\nfig, ax = plt.subplots(1, 1, figsize= (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nSimilarily to Part A, we can see in this plot a visualization of linearly seperable data.\n\ntorch.manual_seed(1234)\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1.0\nloss_vec = []\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nmax_iter = 1000\ncurr_iter = 0\n\nwhile loss &gt; 0 and curr_iter &lt; max_iter:\n    ax = axarr.ravel()[current_ax]\n    \n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    i = torch.randint(n, size = (1,))\n\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = opt.step(x_i, y_i)\n\n    \n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n    \n    curr_iter = curr_iter + 1\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nIn this plot, we can see the evolution of the loss function during training, and the seperating line when the model reaches a loss of zero. In each panel of the plot, we are able to see the loss, the decision boundary that led to that loss and the previous decision boundary. We can see how the loss changed over the course of the iterations, until finally a loss of zero was reached. If this data were not linearly seperable the function would have terminated before the loss function was zero and the final panel would have have had a loss &gt; 0.0.\n\n# Generate 2D Non-Linearly Seperable Data\n# Code modelled from Prof. Phil Chodrow\ntorch.manual_seed(1234)\n\nX, y = perceptron_data(n_points = 300, noise = 0.9, p_dims = 2)\n\nfig, ax = plt.subplots(1, 1, figsize= (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\nBy increasing the noise of the generated perceptron data, we created data that is not linearly-seperable. As visualized in this plot, the data does not appear to be linearly seperable, as the two points overlap pretty significantly and it is not possible to draw a straight line through the two. When use our perceptron on this data we expect it not to reach a loss of zero.\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1.0\nloss_vec = []\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nmax_iter = 1000\ncurr_iter = 0\n\nfinal_w = torch.clone(p.w)\n\nwhile loss &gt; 0 and curr_iter &lt;= max_iter: #and max_iter &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    curr_iter = curr_iter + 1\n    if curr_iter == max_iter:\n        final_w = torch.clone(p.w)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\ndraw_line(final_w, x_min = -2, x_max = 4, ax = ax, color = \"black\")\nax.set_ylim(-2, 4)\n\n\n\n\n\n\n\n\n\n\nThis plot shows the decision boundary of the final iteration. As evidenced in the plot, there and blue and orange points on either side of the boundary, demonstrating that the data is not seperated by the linear line and therefore not linearly seperable.\n\nplt.plot(loss_vec, color = \"skyblue\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"skyblue\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Loss over Perceptron Interations for Non-Linear Data\")\n\n\n\n\n\n\n\n\nThis plot shows the evolution of the loss over training. It does not terminate until it has reached max_iter which makes sense as the data is not linearly seperable.\n\n# Generate 5 feature Data\n# Code modelled from Prof. Phil Chodrow\ntorch.manual_seed(1234)\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims = 5)\n\ntorch.manual_seed(1234)\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter = 1000\n\nwhile loss &gt; 0: #and max_iter &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    max_iter = max_iter - 1\n\n\nplt.plot(loss_vec, color = \"skyblue\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"skyblue\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\", title = \"Loss over Perceptron Interations for 5-feature Data\")\n\n\n\n\n\n\n\n\nThe results of this table indicate that the 5-feature data is linearly seperable because the Perceptron terminates after about 800 iterations instead of reaching max_iter."
  },
  {
    "objectID": "posts/implementing-the-perceptron-alg/index.html#discussion-of-runtime",
    "href": "posts/implementing-the-perceptron-alg/index.html#discussion-of-runtime",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Discussion of Runtime",
    "text": "Discussion of Runtime\nDuring a single iteration of the perceptron algorithm, the runtime is \\(O(n \\times p)\\), where \\(n\\) is the number of data points and \\(p\\) is the number of features. This is due to the calculation of the score function which requires the multiplication of \\(X\\) and \\(w\\) to be calculated. \\(X\\) is size \\(n \\times p\\), as each data point will have \\(p\\) features. Meanwhile \\(w\\) is size \\(n\\).\nTo compute each row of the resulting matrix, takes \\(O(n)\\) time as it requires \\(n\\) multiplcations and \\(n-1\\) additions. As there are \\(p\\) rows in matrix \\(X\\), the total number of operations requires a time complexity of: \\[O(n \\times p)\\].\nThis means that the runtime complexity of a single iteration depends both on the number of features and data points in \\(X\\)."
  },
  {
    "objectID": "posts/implementing-the-perceptron-alg/index.html#conclusion",
    "href": "posts/implementing-the-perceptron-alg/index.html#conclusion",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Conclusion",
    "text": "Conclusion\nThrough this assignment, I was able to learn more about Perceptron and Gradient Descent. Through the experimentation section, I observed that the Perceptron algorithm is only effective when the data is linearly seperable. Its runtime is also dependent on the number of points and number of features in the dataset."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Final Project Blog Post\n\n\n\n\n\nAudio Emotion Classification\n\n\n\n\n\nMay 17, 2024\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nNewton’s Method\n\n\n\n\n\nImplementing Newton’s Method\n\n\n\n\n\nMay 1, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nImplementing the perceptron alg\n\n\n\n\n\nApr 10, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Decision Making\n\n\n\n\n\nOptimal Decision Making\n\n\n\n\n\nApr 1, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nImplementing logistic regression\n\n\n\n\n\nMar 31, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nThe Women in Data Science (WiDS) Conference at Middlebury College\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Adelie, Gentoo, and Chinstrap Penguins.\n\n\n\n\n\nFeb 23, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\nNo matching items"
  }
]