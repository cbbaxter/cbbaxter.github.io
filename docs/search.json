[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The quick brown fox jumped over the lazy dog."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Optimal Decision Making\n\n\n\n\n\nClassifying Adelie, Gentoo, and Chinstrap Penguins.\n\n\n\n\n\nMar 29, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nImplementing the perceptron alg\n\n\n\n\n\nMar 25, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nThe Women in Data Science (WiDS) Conference at Middlebury College\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Adelie, Gentoo, and Chinstrap Penguins.\n\n\n\n\n\nFeb 23, 2023\n\n\nCaitlin Baxter\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n# Load Data\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\nAbstract\nIn this blog post I investigated the classification of Gentoo, Adelie, and Chinstrap penguins. It was determined that using Logistic Regression with the variables culmen length, culmen depth, and the island where the penguins are found allowed for the correct classification of penguin 100.0% of the time on the test data. The culmen is the dorsal ridge atop the bill of the penguin, and its depth and length varies between the three species.\n\nFigure 1: Adelie penguins\nimage source: https://i.natgeofe.com/n/00c35612-9827-40d9-a04e-6079806dca52/3798855_2x1.jpg\n\nFigure 2: Chinstrap penguin\nimage source: https://live.staticflickr.com/707/31973598931_6d9bfd3f18_b.jpg\n\nFigure 3: Gentoo penguin\nimage source: https://cdn.britannica.com/08/152708-050-23B255B3/Chinstrap-penguin.jpg\nLogistic regression was compared with Decision Tree Classification and Support Vector Classification and was determined to be the most accurate model when the C value was set to 17.\n\n# Data Preparation\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nsummary_table = train.groupby([\"Species\", \"Island\"])[\"Body Mass (g)\"].mean()\nprint(summary_table)\n\nSpecies                                    Island   \nAdelie Penguin (Pygoscelis adeliae)        Biscoe       3711.363636\n                                           Dream        3728.888889\n                                           Torgersen    3712.804878\nChinstrap penguin (Pygoscelis antarctica)  Dream        3743.421053\nGentoo penguin (Pygoscelis papua)          Biscoe       5039.948454\nName: Body Mass (g), dtype: float64\n\n\nFrom this summary table it is evident that Chinstrap penguins only live on the Dream island while Gentoo penguins only live on Biscoe. Meanwhile, Adelie Penguins live on Biscoe, Dream, adn Torgersen. This suggests that the Island where an individual is located may be a good variable for a machine learning algorithm as we know that if an individual is located on torgersen they must be an Adelie and that if an individual is found on Dream it is not a Gentoo penguin.\n\nmass_table = sns.relplot(data = train, x = \"Body Mass (g)\" , y = \"Flipper Length (mm)\", hue = \"Species\")\nmass_table.set_axis_labels(\"Body Mass (g)\", \"Flipper Length (mm)\")\nmass_table.add_legend(frameon=True)\nmass_table.legend.set_bbox_to_anchor((1.5, .6))\nplt.gca().set(title = \"Comparing the Body Mass and Flipper Length of Penguins\")\nprint(mass_table)\n\n&lt;seaborn.axisgrid.FacetGrid object at 0x12f8a37f0&gt;\n\n\n\n\n\n\n\n\n\nFrom this figure it is apparent that the flipper length of a penguin is positively related with their body mass. It is clear that Gentoo penguins are generally larger than chinstrap and adelie penguins, and the latter two are relatively similar in size.\n\nflipper_table = sns.relplot(data = train, x = \"Flipper Length (mm)\" , y = \"Culmen Length (mm)\", hue = \"Species\", style = \"Island\")\nflipper_table.set_axis_labels(\"Flipper Length (mm)\", \"Culmen Length (mm)\")\nflipper_table.add_legend(frameon=True)\nplt.gca().set(title = \"Comparing the Culmen Length and Flipper Length of Penguins\")\nflipper_table.legend.set_bbox_to_anchor((1.5, .6))\nprint(flipper_table)\n\n&lt;seaborn.axisgrid.FacetGrid object at 0x154b5fac0&gt;\n\n\n\n\n\n\n\n\n\nIn this plot we can see that while Adelie and Chinstrap penguins have similar Flipper lengths, Chinstrap penguins generally have longer culmens. The flipper and culmen length of Adelie penguins does not appear to vary greatly depending on the island on which they were found.\n\nculmen_table = sns.relplot(data = train, x = \"Culmen Depth (mm)\" , y = \"Culmen Length (mm)\", hue = \"Species\", style = \"Island\")\nculmen_table.set_axis_labels(\"Culmen Depth (mm)\", \"Culmen Length (mm)\")\nculmen_table.add_legend(frameon=True)\nplt.gca().set(title = \"Comparing the Culmen Length and Culmen Depth of Penguins\")\nculmen_table.legend.set_bbox_to_anchor((1.5, .6))\nprint(culmen_table)\n\n&lt;seaborn.axisgrid.FacetGrid object at 0x154959040&gt;\n\n\n\n\n\n\n\n\n\nIn this plot, we observe that the culmen length and culmen depth seem to be generally distinct between species. Through this preliminary analysis, I decided to focus my search on looking at the combination of qualitative variables with the quantitative variables that relate to the size of the penguins, as they appear to vary in body mass, flipper length, and the size of their culmens.\n\nfrom itertools import combinations\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\ndef logisticRegression(X_train, y_train):\n  best_score = 0\n  best_c = 0\n  for c in range(1, 20): \n    LR = LogisticRegression(max_iter = 5000, C = float(c))\n    LR.fit(X_train, y_train)\n    cv_scores_LR = cross_val_score(LR, X_train, y_train, cv = 5)\n    if cv_scores_LR.mean() &gt; best_score:\n      best_score = cv_scores_LR.mean()\n      best_c = c\n  return cv_scores_LR.mean(), best_c\n\ndef decisionTree(X_train, y_train):\n  best_score = 0\n  best_max_depth = 0\n  for i in range(1, 100):\n    DT = DecisionTreeClassifier(max_depth = i)\n    DT.fit(X_train, y_train)\n    cv_scores_DT = cross_val_score(DT, X_train, y_train, cv = 5)\n    if cv_scores_DT.mean() &gt; best_score:\n      best_score = cv_scores_DT.mean()\n      best_max_depth = i\n  return best_score, best_max_depth\n\ndef svc(X_train, y_train):\n  xscaler = preprocessing.StandardScaler().fit(X_train)\n  X_scaled = xscaler.transform(X_train)\n  gammas = 10**np.arange(-5, 5)\n  best_score = 0\n  best_gamma = gammas[0]\n  for gamma in gammas:\n    svc = SVC(gamma = gamma)\n    svc.fit(X_scaled, y_train)\n    cv_scores_svc = cross_val_score(svc, X_scaled, y_train, cv = 5)\n    if cv_scores_svc.mean() &gt; best_score:\n      best_score = cv_scores_svc.mean()\n      best_gamma = gamma\n  return best_score, best_gamma\n\nbest_vars = []\nmax_score = 0\nmodelType = \"\"\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    select_X_train = X_train[cols]\n    \n    # -- Logistic Regression --\n    LR_score, c = logisticRegression(select_X_train, y_train)\n    if LR_score &gt; max_score:\n      max_score = LR_score\n      best_vars = cols\n      modelType = \"LR\"\n      print(max_score, best_vars, modelType, c)\n\n    # -- Decision Tree -- \n    DT_score, max_depth = decisionTree(select_X_train, y_train)\n    if DT_score &gt; max_score:\n      max_score = DT_score\n      best_vars = cols\n      modelType = \"DT\"\n      print(max_score, best_vars, modelType, max_depth)\n   \n    # -- SVC --\n    svc_score, gamma = decisionTree(select_X_train, y_train)\n    if svc_score &gt; max_score:\n      max_score = svc_score\n      best_vars = cols\n      modelType = \"svc\"\n      print(max_score, best_vars, modelType, gamma)\n\nprint(best_vars, max_score, modelType) \n\n\n0.9530920060331824 ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)'] LR 1\n0.9883107088989442 ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] LR 1\n0.9883861236802414 ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)'] LR 18\n1.0 ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] LR 17\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 1.0 LR\n\n\nSimilar to my observations during my initial investigation into the different variables, we see that Island, Culment Length, and Culmen Depth were used in the most accurate model. The Linear Regression model outperformed both the SVC and the Decision Tree. It was determined that when C = 17, the model was 100% accurate. The C is the inverse regularization strength, and the smaller the value specifies stronger regularization (sklearn documentation). C being 17 indicates a weaker regularization and suggests that the model may be focusing on minimizing error in the training data and may not regularize as well. I decided to test the model again with more cross validation.\n\nX_select = X_train[['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']]\nLR = LogisticRegression(max_iter = 5000, C = 1.0)\nLR.fit(X_select, y_train)\ndefault_c_cv_scores_LR = cross_val_score(LR, X_select, y_train, cv = 5)\ndefault_c_cv_scores_LR.mean()\n\nLR = LogisticRegression(max_iter = 5000, C = 17.0)\nLR.fit(X_select, y_train)\ncv_scores_LR = cross_val_score(LR, X_select, y_train, cv = 5)\ncv_scores_LR.mean()\n\nprint(default_c_cv_scores_LR.mean())\nprint(cv_scores_LR.mean())\n\n0.9843891402714933\n1.0\n\n\nWhen I re-ran the Logistic Regression Model with cross validation with the default value of C vs the one that was found to be most effecient in the above function, it is clear that the Logistic Regression model is already fairly accurate at classifying types of penguins using Culmen Depth, Culmen Length, and the Island that they are found.So I decided to test the LR model on the test data.\n\n# Testing the Model on Test Data\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ncols = ['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nX_test, y_test = prepare_data(test)\n\nX_select = X_train[['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']]\nLR = LogisticRegression(max_iter = 5000, C = 1.0)\nLR.fit(X_select, y_train)\n\ny_preds = LR.predict(X_test[cols])\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\n\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_preds, normalize = \"true\")\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\nAs shown in the above confusion matrix the model accurately predicted the species of penguin each time when looking at the Island, the Culmen Length, and Culmen Depth.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\n\nDiscussion\nIt was determined that when classifying Chinstrap, Gentoo, and Adelie penguins, looking at the length and depth of their culmen along with the island where they are found. The model was able to correctly classify the type of penguin 100% of the time. Through a preliminary investigation of the data, it was evident that Island would likely be a helpful variable in determining the species of the penguins. This was supported when Island was selected as one of the variables in the three variable combination for the best accuracy in the for loop. In the confusion matrix, it is evident that all three types of penguins were correctly identified in the test dataset.\nIn this blog post I learned the importance of investigating different values of parameters and how they affect the model. When I was looping through the different variables, I had the script print out the combinations of variables, their accuracy, and the type of model so I could see how it ultimately found the best one. It was interesting to see how changing the value of C in Logisitic Regression could change the model’s ability to accurately classify. For example moving C = 16 to C = 17, boosted the accuracy from 99.2% to 100.0%"
  },
  {
    "objectID": "posts/goal-setting.html",
    "href": "posts/goal-setting.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Caitlin Baxter\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nTwo of these areas that I am especially interested in learning about are the Implementation and the Social responsibility aspects of the course. I am very interested in the social responsibility element of the course. I took CS701 last semester and really enjoyed the introduction to the ethical questions surrounding technology. I would like to learn more about the steps I can take as a computer scientist to reduce sources of bias and harm in machine learning algorithms. In terms of implementation, I would like to be able to understand when to use different types of machine learning algorithms, and how to effectively implement them. I would also like to learn more about the questions to ask before implementation and the tests that can be run to ensure that a machine learning algorithm is implemented in a socially responsible way.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI would like to submit a first draft of all of the blog posts I chose to do within 48 hours of the best-by date. I have assignments due due each week for other classes on Wednesday and Friday mornings, but want to make sure I am staying up-to-date with the assignments and submit the blog posts by the end of the week they are best-by to make sure that I am staying on top of the course material. I would like to do 2 blog posts for each of the course learning objectives: theory, implementation, experimentation, and social responsibility. I also aim to revise all of my blog posts to ‘Meets Expectations’ and have with two of them in the ‘Excellent’ status. I would also like to use blog posts as a way to reflect my learning outside of class as well. I would like to propose and complete an additional blog post on a topic related to algorithmic bias and social responsibility or an interesting way machine learning is being used to solve a problem that I am interested in. Ultimately, my goal for the blog posts is to have a place that reflects my learning over the course of the semester, and that I feel all of my blog posts reflect my level of understanding of the course content.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nIn terms of class attendence, I hope to attend almost every class. Ideally, I aim to only miss class when I am sick or physically incabable of attending. I would like come prepared to present the daily warm-up exercise every class, or at least have made a strong attempt that allows me to ask informed questions of my peers. I would like to participate in a study group outside of class, and work with classmates towards solutions. I find that I am someone who really learns best when I am working with someone else and can bounce ideas off of them and know that explaining things to other people is one of the best ways for me to solidify information. I also aim to keep up to date with current events surrounding technology to make sure that I am aware of the current social and ethical concerns surrounding machine learning. Success in course presence for me basically centres around knowing that I am happy with the amount of effort I have put into the course, and that I have contributed to the overall class dynamic by being a good peer both in class and outside.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nWhile I am not sure exactly what I want to focus on for the final topic, I find the idea of exploring a complex data set in depth or assessing algorithmic bias in a certain class of algorithm to be exciting. My goal for the project is that I select something challenging but fulfilling and that it is a topic I am excited by. I aim to submit all project milestones on time to ensure that I am staying up-to-date and not falling behind. I would like to communicate with my group in a clear and efficient manner, and set regularly scheduled time to meet and work on it as a group. I hope that by meeting with my group regularly, we will be able to divide the tasks in an equal way so as to make sure we all get a lot out of the project, and that too much work does not fall on a single person. I also think that when working on a group project, it is important to set aside time where we can help each other with any problems that we are facing, so I hope to help create an environment where we are comfortable going to each other with questions or to talk things through."
  },
  {
    "objectID": "posts/goal-setting.html#what-youll-learn",
    "href": "posts/goal-setting.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nTwo of these areas that I am especially interested in learning about are the Implementation and the Social responsibility aspects of the course. I am very interested in the social responsibility element of the course. I took CS701 last semester and really enjoyed the introduction to the ethical questions surrounding technology. I would like to learn more about the steps I can take as a computer scientist to reduce sources of bias and harm in machine learning algorithms. In terms of implementation, I would like to be able to understand when to use different types of machine learning algorithms, and how to effectively implement them. I would also like to learn more about the questions to ask before implementation and the tests that can be run to ensure that a machine learning algorithm is implemented in a socially responsible way."
  },
  {
    "objectID": "posts/goal-setting.html#what-youll-achieve",
    "href": "posts/goal-setting.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI would like to submit a first draft of all of the blog posts I chose to do within 48 hours of the best-by date. I have assignments due due each week for other classes on Wednesday and Friday mornings, but want to make sure I am staying up-to-date with the assignments and submit the blog posts by the end of the week they are best-by to make sure that I am staying on top of the course material. I would like to do 2 blog posts for each of the course learning objectives: theory, implementation, experimentation, and social responsibility. I also aim to revise all of my blog posts to ‘Meets Expectations’ and have with two of them in the ‘Excellent’ status. I would also like to use blog posts as a way to reflect my learning outside of class as well. I would like to propose and complete an additional blog post on a topic related to algorithmic bias and social responsibility or an interesting way machine learning is being used to solve a problem that I am interested in. Ultimately, my goal for the blog posts is to have a place that reflects my learning over the course of the semester, and that I feel all of my blog posts reflect my level of understanding of the course content.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nIn terms of class attendence, I hope to attend almost every class. Ideally, I aim to only miss class when I am sick or physically incabable of attending. I would like come prepared to present the daily warm-up exercise every class, or at least have made a strong attempt that allows me to ask informed questions of my peers. I would like to participate in a study group outside of class, and work with classmates towards solutions. I find that I am someone who really learns best when I am working with someone else and can bounce ideas off of them and know that explaining things to other people is one of the best ways for me to solidify information. I also aim to keep up to date with current events surrounding technology to make sure that I am aware of the current social and ethical concerns surrounding machine learning. Success in course presence for me basically centres around knowing that I am happy with the amount of effort I have put into the course, and that I have contributed to the overall class dynamic by being a good peer both in class and outside.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nWhile I am not sure exactly what I want to focus on for the final topic, I find the idea of exploring a complex data set in depth or assessing algorithmic bias in a certain class of algorithm to be exciting. My goal for the project is that I select something challenging but fulfilling and that it is a topic I am excited by. I aim to submit all project milestones on time to ensure that I am staying up-to-date and not falling behind. I would like to communicate with my group in a clear and efficient manner, and set regularly scheduled time to meet and work on it as a group. I hope that by meeting with my group regularly, we will be able to divide the tasks in an equal way so as to make sure we all get a lot out of the project, and that too much work does not fall on a single person. I also think that when working on a group project, it is important to set aside time where we can help each other with any problems that we are facing, so I hope to help create an environment where we are comfortable going to each other with questions or to talk things through."
  },
  {
    "objectID": "posts/optimal-decision-making/index.html",
    "href": "posts/optimal-decision-making/index.html",
    "title": "Optimal Decision Making",
    "section": "",
    "text": "import pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)"
  },
  {
    "objectID": "posts/women-in-data-science/index.html",
    "href": "posts/women-in-data-science/index.html",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "Abstract\nOn March 4, 2024 I attended Middlebury College’s Women in Data Science Conference that aimed to highlight the work of women using data across different disciplines. Part A of this blogpost uses Corbett and Hill’s report “Solving the Equation: The Variables for Women’s Sucess in Engineering and Computing” to discuss the issues facing female representation in the fields of Engineering and Computing and how events that spotlight women can have a hugely positive effect on breaking down biases and building community. In Part B, I reflect on the conference itself, discsussing the different lightning presentations by Middlebury professors and the keynote speaker, Sarah Brown.\n\nPart A: Why Spotlight Women in Data Science?\nWhy is it a problem that women are underrepresented in computing, math, and engineering? For whom is it a problem?\nThe underrepresentation of women in computing, math, and engineering is not only a problem for many women in those fields, but also contributes to larger societal injustices and problems. Diversity in the workforce not only contributes to creativity, productivity, and innovation, but by excluding approximately half of the population from engineering and technical decisions societal inequalities are often perpetuated. This winter, I began reading a book called Invisible Women: Data Bias in a World Designed for Men by Caroline Criado-Perez that discussed the disparity in the data that has led to many of accepted societal designs and standards. In it, they include a quote from Simone de Beauvoir’s The Second Sex, that states “Representation of the world, like the world itself, is the work of men; they describe it from their own point of view, which they confuse with the absolute truth”. This idea is highlighted throughout Corbett and Hill’s discussion of the negative impact of the lack of representation of women in engineering and computing fields in their report “Solving the Equation: The Variables for Women’s Success in Engineering and Computing”. The lack of representation also makes it more difficult for women who are entering the field. With a lack of role models or representation in high power positions, it can be difficult for women to feel confident or like they belong in computing or engineering fields. This in-turn continues to propagate and negatively reinforce the issues surrounding the underrepresentation in the first place.\nHow is the representation and status of women in computing today different from the 1950s and 1960s? What are some of the forces that brought this change?\nSince the 1950s and 60s women have made substantial advances in many STEM fields, making up 39% of chemists in 2013 as opposed to 8% in 1960. In Computing fields however, women make up just over a quarter of the workforce which is approximately the same as it was in 1960. Only 12% of engineers are women, which is a substantial increase from the 1% in 1960, yet still remains a strikingly small percentage. An important way to facilitate the increase of female representation is by changing the environment. Corbett and Hill discuss how Harvey Mudd College was able to increase the percentage of women graduating from its computing program from 12% to 40% in 5 years by making three changes that made their courses a more welcoming and accessible environment for female computer scientists. Recognizing that the problem not only lies with attracting women to a program, but also keeping them invested and involved, is another essential element of representation.\nWhich of the barriers and unequal challenges described in the section “Why So Few” can be eroded by events that spotlight the achievements of women in STEM?\nA large barrier for women in Computing fields is the lack of a perceived community, both in their role, which is stereotypically often perceived as very solitary, and as an underrepresented group of people. Women are more likely than men to express an interest in work with a clear social purpose and as Corbett and Hill write, “Incorporating communal aspects - both in messaging and substance - into engineering and computing work will likely increase the appeal of these fields to communally oriented people, many of whom are women.” Events that highlight the success of women also help cultivate a sense of belonging which is essential for a field or workplace when trying to attract and retain diversity. Through events that spotlight the achievements of women, we are able to also help erode implicit gender biases and stereotypes. Corbett and Hill discuss how exposing students to female role models early in life might be able to change the implicit biases between math capabilities and gender.\n\n\nPart B: Middlebury Women in Data Science Conference\nMiddlebury professor Amy Yuen presented on ‘Is the UN Security Council a Democratic Institution?’ and discussed how she has used data in her research on the UN’s security council and how the status of different members effect the resolutions and types of meetings the council holds. The UN Security council is comprised of 15 members, with 5 permanent members who hold veto power, and 10 elected members that serve 2-year terms. In measuring the different resolutions and meetings brought forward by the permanent members versus the elected members, Professor Yuen looked to access how the different elected members affect the council. They found differences in sponsorship of resolutions, determining that permanent members often put out the most resolutions that relate to current events while elected members focus on more thematic issues such as climate change. They found that the actual line of best fit of their data fit closer to their modelled ‘most inclusive cycle’ which suggests that while the Security Council is definitely not democratic in the way we often think of it, it is fairly representative of the world scene and engages more states than people would imagine. I learned a lot about how the UN Security Council functions from this presentation and also a fun example of how data is used outside of Computer Science research. One thing that I found interesting was the challenges of aquiring data. Professor Yuen discussed how even though all of this data is public record and has been well-documented, the data provided by the UN was at times incorrect and had to be re-checked by her research assistants.\nGeography professor Dr. Jessica L’Roe presented on her research on deforestation. She showed us several of her projects, and discussed how she and her research assitance both collected data and analyzed it. One of the first examples she discussed was in Brazil, where while the government was very good at detecting deforestation, they found it difficult to actually identify who was responsible for clearing land. The government created a new initiative that had people register their land, but the self-reports provided by people were often incorrect as people would lie about the size of their land. So while they have a lot of data from aerial observation and statistics, a lot of their research relies on interviewing other people. She echoed this in another example of her research in Uganda. I found her discussion of how interviewing of locals provided them with essential information in wildlife and woodlots conservation. I think that sometimes in Computer Science and Statistics data can feel really impersonal and it was nice to bring the discussion back to the people who are the basis of and are greatly affected by models.\nFinally, Professor Laura Biester presented on her research using Computation Linguistic Models to study Mental Health issues. Research into mental health involves the fundamental challenge of access to quality data due to the privacy protections in place for health data, so many researchers rely on self-reports where an individual has posted on the internet about their diagnosis. Typically, models built on self-report datasets do not generalize as well, so Professor Biester and her colleagues turned to Reddit. Looking at over 360 JSON files with all of the Redditposts and comments from 2006-2019, they developed a dataset with 20k diagnosed users and controls then used Bidirectional Encoder Representations from Transformers (BERT) to extract contextual representations of words that were associated with the lead-up to a depression diagnosis. I learned a lot about how these large language model studies are conducted through Professor Biester’s presentation. I found her discussion on the limitations of Reddit data as it includes very little demographic information about the individuals to be interesting, and made me wonder at how the results might vary if we had information about the gender of the user. How would the terminology vary for a female identifying, or non-binary individual? I found Professor Biester’s discussion on how she uses huge amounts of data to investigate language to be very interesting and informative.\nProfessor Sarah Brown, from the University of Rhode Island, was the main keynote speaker of the event. She presented on ‘Data Science Skills in Unexpected Places’ and discussed different non-technical skills that she has aquired over the years that help her solve data science problems and overcome roadblocks when faced with an issue. One of her main points that I found particularly interesting was her discussion of data as a primary source that needs to be assessed within its context in order to be properly understood. She began her presentation by showing a three circled-venn diagram that had ‘Computer Science’, ‘Domain Experts’, and ‘Statistics’ in each circle, with the overlapping section highlighted as ‘Data Science’. She discussed how each discipline is not simply a topic, but rather a community full of people, norms, and vocabularies that need to be understood in order to properly analyze its data.\nProfessor Brown also discussed the question of accuracy versus fairness, suggesting a change from the traditional data pipeline to incorporate the question of fairness into the earlier stages when one is exploring the data and variables:\ncollect -&gt; clean -&gt; explore -&gt; fair? -&gt; model -&gt; deploy\nShe discussed an example in healthcare where by changing the target variable of the model, they were able to make it more equitable. Computer Science is a novice field with disproportionate power, which can lead to chaos. By incorporating discussions of fairness and auditing our models into our processes we are able to work on addressing bias in models as we are going instead of realizing after deployment that it is behaving problematically. One thing that I found particularily impactful was when Professor Brown stated how we can have an incredible accurate algorithm that is biased, because the world is biased and therefore the data generated by the world is biased.\nReflection\nAs I was scrolling through Instagram the other day I came across a commercial that was made by Mercedes-Benz for International Women’s Day in 2023. The video is narrated by a few young girls and the overall premise is that they are not interested in being ‘exceptional’ because they are the first woman to do something, they would rather be one of many. I think that messaging really highlights what I have been reflecting on throughout this blogpost - how the underrepresentation of women in Computing and Engineering fields can feel like a self-perpetuating cycle, and that by highlighting and empowering successful women in these fields we are able to build representation and community. I had a great time at Middlebury’s Women in Data Science conference and felt that I both learned a lot, and also got to meet a lot of other students through their ‘Get to know you bingo activity’ and left happily with a midd data tshirt."
  },
  {
    "objectID": "posts/implementing-the-perceptron-alg/index.html",
    "href": "posts/implementing-the-perceptron-alg/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n# Generate Data\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n    y = y.type(torch.FloatTensor)\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n# Check your Implementation \n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0.0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")"
  }
]