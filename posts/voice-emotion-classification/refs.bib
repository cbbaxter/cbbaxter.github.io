@misc{goy_huiwen_hearing_nodate,
   author = {{Goy Huiwen} and {M. Kathleen Pichora-Fuller} and {Gurjit Singh} and {Frank A. Russo}},
   title = {Hearing {Aids} {Benefit} {Recognition} of {Words} in {Emotional} {Speech} but {Not} {Emotion} {Identification}},
   url = {https://journals.sagepub.com/doi/pdf/10.1177/2331216518801736},
   urldate = {2024-05-06},
   bdsk-url-1 = {https://journals.sagepub.com/doi/pdf/10.1177/2331216518801736}}


@incollection{cambria_affective_2017,
   abstract = {Understanding emotions is one of the most important aspects of personal development and growth and, as such, it is a key tile for the emulation of human intelligence. Besides being a important for the advancement of AI, emotion processing is also important for the closely related task of polarity detection. The opportunity automatically to capture the sentiments of the general public about social events, political movements, marketing campaigns, and product preferences, in fact, has raised increasing interest both in the scientific community, for the exciting open challenges, and in the business world, for the remarkable fallouts in marketing and financial market prediction. This has led to the emerging fields of affective computing and sentiment analysis, which leverage on human-computer interaction, information retrieval, and multimodal signal processing for distilling people's sentiments from the ever-growing amount of online social data.},
   address = {Cham},
   author = {Cambria, Erik and Das, Dipankar and Bandyopadhyay, Sivaji and Feraco, Antonio},
   booktitle = {A {Practical} {Guide} to {Sentiment} {Analysis}},
   doi = {10.1007/978-3-319-55394-8_1},
   editor = {Cambria, Erik and Das, Dipankar and Bandyopadhyay, Sivaji and Feraco, Antonio},
   file = {Full Text PDF:/Users/otismilliken/Zotero/storage/3Z2FGN7X/Cambria et al. - 2017 - Affective Computing and Sentiment Analysis.pdf:application/pdf},
   isbn = {978-3-319-55394-8},
   keywords = {Affective computing, Five eras of the Web, Hybrid approaches, Jumping NLP curves, Sentiment analysis},
   language = {en},
   pages = {1--10},
   publisher = {Springer International Publishing},
   title = {Affective {Computing} and {Sentiment} {Analysis}},
   url = {https://doi.org/10.1007/978-3-319-55394-8_1},
   urldate = {2024-05-06},
   year = {2017},
   bdsk-url-1 = {https://doi.org/10.1007/978-3-319-55394-8_1}}




@article{zhang_acoustic_2021,
   abstract = {Recently, excellent performance has been achieved in Acoustic Scene Classification (ASC) by using Convolutional Neural Networks (CNNs) and Mel spectrogram feature representations. The utilization of Mel spectrogram feature is attracting increasing attention for its effectiveness in improving the performance. In this paper, Gradient-weighted Class Activation Mapping (Grad-CAM), a CNN visualization technique, evaluates what information is perceived by a CNN. The importance of the regions in the Mel spectrogram varies significantly for the trained CNN. Some areas are significantly activated, some are not. Because the whole Mel spectrogram contains a large amount of information, some information will not take effect when the entire Mel spectrogram is fed into a CNN simultaneously, which leaves some leeway to improve the feature utilization of the Mel spectrogram. This paper proposed a method based on spectrogram decomposing and model merging to make local features more prominent and make CNN easier to train. Specifically, a whole Mel spectrogram is segmented along the time and frequency dimensions and then generates multiple sub-spectrograms. The sub-spectrograms in the same frequency bins share the same CNN sub-model. Then the prediction of the whole Mel spectrogram is obtained by merging the outputs of CNN sub-models. The experiment results show that our proposed algorithm outperforms the existing systems by 5.64\%. Also, the results of confusion matrices and class activation maps demonstrate the effectiveness of Mel spectrogram decomposition.},
   author = {Zhang, Tao and Feng, Guoqing and Liang, Jinhua and An, Tong},
   doi = {https://doi.org/10.1016/j.apacoust.2021.108258},
   issn = {0003-682X},
   journal = {Applied Acoustics},
   keywords = {Acoustic scene classification, Feature utilization, Mel spectrogram, Model merging},
   pages = {108258},
   title = {Acoustic scene classification based on {Mel} spectrogram decomposition and model merging},
   url = {https://www.sciencedirect.com/science/article/pii/S0003682X21003522},
   volume = {182},
   year = {2021},
   bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0003682X21003522},
   bdsk-url-2 = {https://doi.org/10.1016/j.apacoust.2021.108258}}




@article{ong_mel-mvitv2_2023,
   abstract = {Speech emotion recognition aims to automatically identify and classify emotions from speech signals. It plays a crucial role in various applications such as human-computer interaction, affective computing, and social robotics. Over the years, researchers have proposed different approaches for speech emotion recognition, leveraging various classifiers and features. However, despite the advancements, existing methods in speech emotion recognition still have certain limitations. Some approaches rely on handcrafted features that may not capture the full complexity of emotional information present in speech signals, while others may suffer from a lack of robustness and generalization when applied to different datasets. To address these challenges, this paper proposes a speech emotion recognition method that combines Mel spectrogram with Short-Term Fourier Transform (Mel-STFT) and the Improved Multiscale Vision Transformers (MViTv2). The Mel-STFT spectrograms capture both the frequency and temporal information of speech signals, providing a more comprehensive representation of the emotional content. The MViTv2 classifier introduces multi-scale visual modeling with different stages and pooling attention mechanisms. MViTv2 incorporates relative positional embeddings and a residual pooling connection to effectively model the interactions between tokens in the space-time structure, preserve essential information, and improve the efficiency of the model. Experimental results demonstrate that the proposed method generalizes well on different datasets, achieving an accuracy of 91.51\% on the Emo-DB dataset, 81.75\% on the RAVDESS dataset, and 64.03\% on the IEMOCAP dataset.},
   author = {Ong, Kah Liang and Lee, Chin Poo and Lim, Heng Siong and Lim, Kian Ming and Alqahtani, Ali},
   doi = {10.1109/ACCESS.2023.3321122},
   file = {IEEE Xplore Abstract Record:/Users/otismilliken/Zotero/storage/8A9HTQQL/10268420.html:text/html;IEEE Xplore Full Text PDF:/Users/otismilliken/Zotero/storage/RQVT24VA/Ong et al. - 2023 - Mel-MViTv2 Enhanced Speech Emotion Recognition Wi.pdf:application/pdf},
   issn = {2169-3536},
   journal = {IEEE Access},
   keywords = {Emo-DB, Emotion recognition, Feature extraction, IEMOCAP, improved multiscale vision transformers, Mel frequency cepstral coefficient, mel spectrogram, mel spectrogram with short-time Fourier transform, RAVDESS, spectrogram, Spectrogram, Speech, speech emotion, speech emotion recognition, Speech enhancement, Speech recognition, Transformers, vision transformer},
   note = {Conference Name: IEEE Access},
   pages = {108571--108579},
   shorttitle = {Mel-{MViTv2}},
   title = {Mel-{MViTv2}: {Enhanced} {Speech} {Emotion} {Recognition} {With} {Mel} {Spectrogram} and {Improved} {Multiscale} {Vision} {Transformers}},
   url = {https://ieeexplore.ieee.org/document/10268420},
   urldate = {2024-05-06},
   volume = {11},
   year = {2023},
   bdsk-url-1 = {https://ieeexplore.ieee.org/document/10268420},
   bdsk-url-2 = {https://doi.org/10.1109/ACCESS.2023.3321122}},


@article{zeng_spectrogram_2019,
   abstract = {Audio classification is regarded as a great challenge in pattern recognition. Although audio classification tasks are always treated as independent tasks, tasks are essentially related to each other such as speakers' accent and speakers' identification. In this paper, we propose a Deep Neural Network (DNN)-based multi-task model that exploits such relationships and deals with multiple audio classification tasks simultaneously. We term our model as the gated Residual Networks (GResNets) model since it integrates Deep Residual Networks (ResNets) with a gate mechanism, which extract better representations between tasks compared with Convolutional Neural Networks (CNNs). Specifically, two multiplied convolutional layers are used to replace two feed-forward convolution layers in the ResNets. We tested our model on multiple audio classification tasks and found that our multi-task model achieves higher accuracy than task-specific models which train the models separately.},
   author = {Zeng, Yuni and Mao, Hua and Peng, Dezhong and Yi, Zhang},
   doi = {10.1007/s11042-017-5539-3},
   file = {Zeng et al. - 2019 - Spectrogram based multi-task audio classification.pdf:/Users/otismilliken/Zotero/storage/GLV7PA9B/Zeng et al. - 2019 - Spectrogram based multi-task audio classification.pdf:application/pdf},
   issn = {1380-7501, 1573-7721},
   journal = {Multimedia Tools and Applications},
   language = {en},
   month = feb,
   number = {3},
   pages = {3705--3722},
   title = {Spectrogram based multi-task audio classification},
   url = {http://link.springer.com/10.1007/s11042-017-5539-3},
   urldate = {2024-05-17},
   volume = {78},
   year = {2019},
   bdsk-url-1 = {http://link.springer.com/10.1007/s11042-017-5539-3},
   bdsk-url-2 = {https://doi.org/10.1007/s11042-017-5539-3}},


@article{livingstone_ryerson_2018,
   abstract = {The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.},
   author = {Livingstone, Steven R. and Russo, Frank A.},
   doi = {10.1371/journal.pone.0196391},
   file = {Full Text PDF:/Users/otismilliken/Zotero/storage/D86R2NKI/Livingstone and Russo - 2018 - The Ryerson Audio-Visual Database of Emotional Spe.pdf:application/pdf},
   issn = {1932-6203},
   journal = {PLOS ONE},
   keywords = {Emotions, Face, Facial expressions, Fear, Musculoskeletal mechanics, Music cognition, Speech, Vocalization},
   language = {en},
   month = may,
   note = {Publisher: Public Library of Science},
   number = {5},
   pages = {e0196391},
   shorttitle = {The {Ryerson} {Audio}-{Visual} {Database} of {Emotional} {Speech} and {Song} ({RAVDESS})},
   title = {The {Ryerson} {Audio}-{Visual} {Database} of {Emotional} {Speech} and {Song} ({RAVDESS}): {A} dynamic, multimodal set of facial and vocal expressions in {North} {American} {English}},
   url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196391},
   urldate = {2024-05-17},
   volume = {13},
   year = {2018},
   bdsk-url-1 = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196391},
   bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0196391}}